using device 0 NVIDIA GeForce RTX 3090
DEVICE: cuda
client_num: 2
MPC模式：每个客户端持有 716 个特征维度
客户端 0: 特征维度 [0:716]
客户端 1: 特征维度 [716:1433]
tensor(0.3528, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0095, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1248, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0621, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0843, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1097, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1078, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0888, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0678, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0520, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0428, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0407, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0439, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0478, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0481, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0441, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0383, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0032, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0331, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0032, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0292, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0262, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0246, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0246, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0254, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0256, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0244, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0223, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0204, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0190, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0181, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0032, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0176, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0174, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0175, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0174, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0167, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0158, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0150, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0144, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0141, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0139, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0139, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0138, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0135, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0131, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0127, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0124, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0122, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0121, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0120, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0119, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0117, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0115, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.3828, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0130, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1296, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0127, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0805, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0120, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1136, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0109, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1350, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0093, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1224, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0078, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0949, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0065, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0716, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0581, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0535, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0551, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0589, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0603, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0566, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0494, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0422, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0374, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0349, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0336, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0330, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0328, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0325, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0311, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0288, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0266, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0252, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0245, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0239, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0234, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0231, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0227, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0220, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0209, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0199, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0193, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0190, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0188, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0185, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0182, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0178, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0174, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0169, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0166, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0164, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0162, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0160, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0158, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0155, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0054, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0153, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0054, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0150, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)
tran.0.fc1.weight
tran.0.fc1.bias
tran.1.fc1.weight
tran.1.fc1.bias
classifier.fc1.weight
classifier.fc1.bias
classifier.fc2.weight
classifier.fc2.bias
classifier.layernorm.weight
classifier.layernorm.bias
tran.0.fc1.weight
tran.0.fc1.bias
tran.1.fc1.weight
tran.1.fc1.bias
classifier.fc1.weight
classifier.fc1.bias
classifier.fc2.weight
classifier.fc2.bias
classifier.layernorm.weight
classifier.layernorm.bias
  0%|          | 0/300 [00:00<?, ?it/s]tensor(0.2854, dtype=torch.float64) 0.12670656850109172
tensor(0.2497, dtype=torch.float64) 0.13063892283186995
tensor(0.3001, dtype=torch.float64) 0.1834748866245766
tensor(0.1685, dtype=torch.float64) 0.0797332875527224
tensor(0.5166, dtype=torch.float64) 0.4445340942846844
tensor(0.3887, dtype=torch.float64) 0.2879347556368741
tensor(0.3850, dtype=torch.float64) 0.24254929655945484
tensor(0.3309, dtype=torch.float64) 0.20609344140908992
tensor(0.3739, dtype=torch.float64) 0.24860414094000147
tensor(0.4317, dtype=torch.float64) 0.3451734788672148
tensor(0.4846, dtype=torch.float64) 0.381739334868297
tensor(0.4674, dtype=torch.float64) 0.39901971818770604
tensor(0.5141, dtype=torch.float64) 0.45303311472487506
tensor(0.5412, dtype=torch.float64) 0.463682890660365
tensor(0.4945, dtype=torch.float64) 0.43478050611271374
tensor(0.4686, dtype=torch.float64) 0.401146757988072
tensor(0.4686, dtype=torch.float64) 0.3989751422159781
tensor(0.4367, dtype=torch.float64) 0.3725779093510915
tensor(0.4305, dtype=torch.float64) 0.363436542479041
tensor(0.4588, dtype=torch.float64) 0.39032366849007927
  7%|▋         | 20/300 [00:00<00:01, 195.07it/s]tensor(0.5018, dtype=torch.float64) 0.43255235264143704
tensor(0.5387, dtype=torch.float64) 0.4777601643168776
tensor(0.5633, dtype=torch.float64) 0.5053497547474848
tensor(0.5658, dtype=torch.float64) 0.5106302575704992
tensor(0.5621, dtype=torch.float64) 0.5055346032148536
tensor(0.5597, dtype=torch.float64) 0.5109895842231668
tensor(0.5781, dtype=torch.float64) 0.543800394250587
tensor(0.5855, dtype=torch.float64) 0.5566031167511503
tensor(0.5683, dtype=torch.float64) 0.5416149585187736
tensor(0.5572, dtype=torch.float64) 0.5309916091003958
tensor(0.5474, dtype=torch.float64) 0.5242202109890404
tensor(0.5314, dtype=torch.float64) 0.5029219503489292
tensor(0.5437, dtype=torch.float64) 0.5152289471270923
tensor(0.5621, dtype=torch.float64) 0.5302689559325152
tensor(0.5941, dtype=torch.float64) 0.5532055687661017
tensor(0.6101, dtype=torch.float64) 0.5701043870263572
tensor(0.6175, dtype=torch.float64) 0.5768978036795258
tensor(0.6212, dtype=torch.float64) 0.581778488080644
tensor(0.6125, dtype=torch.float64) 0.5735605801988579
tensor(0.6089, dtype=torch.float64) 0.570876817550836
tensor(0.6027, dtype=torch.float64) 0.5647788375795234
tensor(0.5978, dtype=torch.float64) 0.5626545959848606
 14%|█▍        | 42/300 [00:00<00:01, 204.25it/s]tensor(0.6002, dtype=torch.float64) 0.5719305670560344
tensor(0.6052, dtype=torch.float64) 0.581120017810614
tensor(0.6224, dtype=torch.float64) 0.6066379414160283
tensor(0.6298, dtype=torch.float64) 0.6157570259765086
tensor(0.6408, dtype=torch.float64) 0.6295361892028146
tensor(0.6593, dtype=torch.float64) 0.6499658564544644
tensor(0.6642, dtype=torch.float64) 0.6557971941989881
tensor(0.6704, dtype=torch.float64) 0.6618242160047664
tensor(0.6740, dtype=torch.float64) 0.6635710755727124
tensor(0.6691, dtype=torch.float64) 0.6572016904440079
tensor(0.6691, dtype=torch.float64) 0.6574759738508902
tensor(0.6642, dtype=torch.float64) 0.6501226308166506
tensor(0.6654, dtype=torch.float64) 0.6509470456639715
tensor(0.6605, dtype=torch.float64) 0.6461341409488004
tensor(0.6581, dtype=torch.float64) 0.6442631165603028
tensor(0.6531, dtype=torch.float64) 0.6381595144363124
tensor(0.6531, dtype=torch.float64) 0.6394489210205225
tensor(0.6581, dtype=torch.float64) 0.6454739630856241
tensor(0.6630, dtype=torch.float64) 0.6509397196098645
tensor(0.6679, dtype=torch.float64) 0.6581235533251578
tensor(0.6765, dtype=torch.float64) 0.6678820514520001
 21%|██        | 63/300 [00:00<00:01, 195.52it/s]tensor(0.6827, dtype=torch.float64) 0.6754689012887409
tensor(0.6839, dtype=torch.float64) 0.6768237119673025
tensor(0.6839, dtype=torch.float64) 0.6765523512528125
tensor(0.6863, dtype=torch.float64) 0.6786707084806266
tensor(0.6863, dtype=torch.float64) 0.678546301617656
tensor(0.6827, dtype=torch.float64) 0.674773781888947
tensor(0.6814, dtype=torch.float64) 0.6730608220524065
tensor(0.6753, dtype=torch.float64) 0.6655370460389636
tensor(0.6814, dtype=torch.float64) 0.6718412046857027
tensor(0.6863, dtype=torch.float64) 0.6775305202349928
tensor(0.6876, dtype=torch.float64) 0.6794992963369881
tensor(0.6888, dtype=torch.float64) 0.68063453020369
tensor(0.6863, dtype=torch.float64) 0.6784698713878519
tensor(0.6839, dtype=torch.float64) 0.6761448097634877
tensor(0.6827, dtype=torch.float64) 0.6741245072370825
tensor(0.6777, dtype=torch.float64) 0.6695071106593643
tensor(0.6740, dtype=torch.float64) 0.6657557995427771
tensor(0.6728, dtype=torch.float64) 0.6641430874057481
tensor(0.6728, dtype=torch.float64) 0.6641868634118622
tensor(0.6753, dtype=torch.float64) 0.6672220608497148
 28%|██▊       | 83/300 [00:00<00:01, 185.06it/s]tensor(0.6814, dtype=torch.float64) 0.6741692146047015
tensor(0.6863, dtype=torch.float64) 0.6803848149303826
tensor(0.6863, dtype=torch.float64) 0.680843919896711
tensor(0.6876, dtype=torch.float64) 0.6820757687391013
tensor(0.6888, dtype=torch.float64) 0.6826198069512802
tensor(0.6863, dtype=torch.float64) 0.6786347724858104
tensor(0.6802, dtype=torch.float64) 0.6720826160062539
tensor(0.6765, dtype=torch.float64) 0.6682659137911945
tensor(0.6753, dtype=torch.float64) 0.6665665660812496
tensor(0.6753, dtype=torch.float64) 0.6668612207724842
tensor(0.6753, dtype=torch.float64) 0.6670721889961805
tensor(0.6753, dtype=torch.float64) 0.6673149853718526
tensor(0.6777, dtype=torch.float64) 0.6702943546780294
tensor(0.6814, dtype=torch.float64) 0.6742257392114629
tensor(0.6839, dtype=torch.float64) 0.6766129193907615
tensor(0.6900, dtype=torch.float64) 0.6833994449997476
tensor(0.6900, dtype=torch.float64) 0.6842343286805405
tensor(0.6863, dtype=torch.float64) 0.6806371872199141
tensor(0.6814, dtype=torch.float64) 0.6753585300274408
tensor(0.6753, dtype=torch.float64) 0.6684380808111623
tensor(0.6704, dtype=torch.float64) 0.662935879539104
 35%|███▍      | 104/300 [00:00<00:01, 189.80it/s]tensor(0.6704, dtype=torch.float64) 0.6626267410281297
tensor(0.6704, dtype=torch.float64) 0.6632180843020297
tensor(0.6704, dtype=torch.float64) 0.6627720125661153
tensor(0.6728, dtype=torch.float64) 0.6650640775725081
tensor(0.6765, dtype=torch.float64) 0.6689675057149342
tensor(0.6802, dtype=torch.float64) 0.6732383260808026
tensor(0.6802, dtype=torch.float64) 0.6739767925792344
tensor(0.6814, dtype=torch.float64) 0.6748094240235956
tensor(0.6765, dtype=torch.float64) 0.6691835944299698
tensor(0.6740, dtype=torch.float64) 0.6667431304768913
tensor(0.6765, dtype=torch.float64) 0.6694523389704544
tensor(0.6753, dtype=torch.float64) 0.6683510886632389
tensor(0.6740, dtype=torch.float64) 0.6686946182729242
tensor(0.6802, dtype=torch.float64) 0.6749473576178686
tensor(0.6814, dtype=torch.float64) 0.6764482757543783
tensor(0.6814, dtype=torch.float64) 0.6767509785404874
tensor(0.6839, dtype=torch.float64) 0.6794474341324082
tensor(0.6839, dtype=torch.float64) 0.6780510829964316
tensor(0.6839, dtype=torch.float64) 0.6778385658552182
tensor(0.6814, dtype=torch.float64) 0.674434609908911
 41%|████▏     | 124/300 [00:00<00:00, 178.22it/s]tensor(0.6814, dtype=torch.float64) 0.6745402085436141
tensor(0.6827, dtype=torch.float64) 0.6764634403876053
tensor(0.6814, dtype=torch.float64) 0.6749310707656225
tensor(0.6790, dtype=torch.float64) 0.6721119679628457
tensor(0.6790, dtype=torch.float64) 0.6722963662870176
tensor(0.6814, dtype=torch.float64) 0.6746624567067372
tensor(0.6814, dtype=torch.float64) 0.6753395630692767
tensor(0.6876, dtype=torch.float64) 0.6826986230879138
tensor(0.6900, dtype=torch.float64) 0.6847687293851761
tensor(0.6888, dtype=torch.float64) 0.6830329187013935
tensor(0.6839, dtype=torch.float64) 0.677506953829389
tensor(0.6876, dtype=torch.float64) 0.6805633634976882
tensor(0.6851, dtype=torch.float64) 0.6779112454267017
tensor(0.6802, dtype=torch.float64) 0.6726406094221289
tensor(0.6851, dtype=torch.float64) 0.6786208881034638
tensor(0.6814, dtype=torch.float64) 0.6748448099193621
tensor(0.6863, dtype=torch.float64) 0.6798938035064083
tensor(0.6839, dtype=torch.float64) 0.6782633532925478
tensor(0.6863, dtype=torch.float64) 0.6813956073842654
 48%|████▊     | 143/300 [00:00<00:00, 180.19it/s]tensor(0.6839, dtype=torch.float64) 0.6785047216575022
tensor(0.6753, dtype=torch.float64) 0.6686148910562234
tensor(0.6765, dtype=torch.float64) 0.669050874245213
tensor(0.6740, dtype=torch.float64) 0.6662789748970622
tensor(0.6753, dtype=torch.float64) 0.6670377261847968
tensor(0.6777, dtype=torch.float64) 0.6709744449331403
tensor(0.6777, dtype=torch.float64) 0.6715089824655277
tensor(0.6827, dtype=torch.float64) 0.6763245047294114
tensor(0.6839, dtype=torch.float64) 0.6780149526173815
tensor(0.6851, dtype=torch.float64) 0.6799642893919107
tensor(0.6863, dtype=torch.float64) 0.6815069128488063
tensor(0.6827, dtype=torch.float64) 0.6773544200302166
tensor(0.6753, dtype=torch.float64) 0.6692498906810551
tensor(0.6790, dtype=torch.float64) 0.6727089124071154
tensor(0.6851, dtype=torch.float64) 0.6795859453700624
tensor(0.6802, dtype=torch.float64) 0.6743566038038059
tensor(0.6790, dtype=torch.float64) 0.6732664469310999
tensor(0.6814, dtype=torch.float64) 0.6748377021321865
tensor(0.6827, dtype=torch.float64) 0.6758178477578799
tensor(0.6802, dtype=torch.float64) 0.6732977952372954
tensor(0.6814, dtype=torch.float64) 0.6751141645055715
tensor(0.6765, dtype=torch.float64) 0.6702825214359813
 55%|█████▌    | 165/300 [00:00<00:00, 190.88it/s]tensor(0.6814, dtype=torch.float64) 0.6753524558292753
tensor(0.6765, dtype=torch.float64) 0.6705387774987757
tensor(0.6728, dtype=torch.float64) 0.6665816916661739
tensor(0.6740, dtype=torch.float64) 0.6673541967044668
tensor(0.6704, dtype=torch.float64) 0.6636357805640257
tensor(0.6777, dtype=torch.float64) 0.6713525797836696
tensor(0.6839, dtype=torch.float64) 0.6772798520050843
tensor(0.6876, dtype=torch.float64) 0.6811747108331847
tensor(0.6839, dtype=torch.float64) 0.6774020269858934
tensor(0.6814, dtype=torch.float64) 0.6748428041899169
tensor(0.6753, dtype=torch.float64) 0.6678247567202653
tensor(0.6790, dtype=torch.float64) 0.6721373397585081
tensor(0.6802, dtype=torch.float64) 0.6741675334497956
tensor(0.6802, dtype=torch.float64) 0.6741717588615785
tensor(0.6851, dtype=torch.float64) 0.6790415315092128
tensor(0.6827, dtype=torch.float64) 0.6763868337814444
tensor(0.6814, dtype=torch.float64) 0.6756476508697971
tensor(0.6839, dtype=torch.float64) 0.6785610223409709
tensor(0.6839, dtype=torch.float64) 0.6787926713474476
tensor(0.6888, dtype=torch.float64) 0.683279445940836
tensor(0.6888, dtype=torch.float64) 0.6833049849428481
tensor(0.6863, dtype=torch.float64) 0.6799407126215128
 62%|██████▏   | 187/300 [00:00<00:00, 198.41it/s]tensor(0.6876, dtype=torch.float64) 0.6812629874448592
tensor(0.6863, dtype=torch.float64) 0.6803894466575692
tensor(0.6876, dtype=torch.float64) 0.6819153468299547
tensor(0.6839, dtype=torch.float64) 0.6794765565906173
tensor(0.6814, dtype=torch.float64) 0.6773091060021496
tensor(0.6777, dtype=torch.float64) 0.6727527139802247
tensor(0.6740, dtype=torch.float64) 0.6688930762573266
tensor(0.6740, dtype=torch.float64) 0.6690531312628422
tensor(0.6728, dtype=torch.float64) 0.6672804699477691
tensor(0.6740, dtype=torch.float64) 0.6683471135117957
tensor(0.6802, dtype=torch.float64) 0.6745832630529331
tensor(0.6851, dtype=torch.float64) 0.6793271582906169
tensor(0.6925, dtype=torch.float64) 0.6879077058218883
tensor(0.6913, dtype=torch.float64) 0.6863676228098634
tensor(0.6814, dtype=torch.float64) 0.6754075861070024
tensor(0.6790, dtype=torch.float64) 0.673218148773637
tensor(0.6790, dtype=torch.float64) 0.6725972321585257
tensor(0.6753, dtype=torch.float64) 0.6689761339512459
tensor(0.6765, dtype=torch.float64) 0.6702493174825767
tensor(0.6765, dtype=torch.float64) 0.6701309560403952
tensor(0.6777, dtype=torch.float64) 0.6713470056525963
tensor(0.6814, dtype=torch.float64) 0.675660814640238
 70%|██████▉   | 209/300 [00:01<00:00, 203.54it/s]tensor(0.6839, dtype=torch.float64) 0.678607611533174
tensor(0.6851, dtype=torch.float64) 0.6796364092724574
tensor(0.6839, dtype=torch.float64) 0.678358531714349
tensor(0.6790, dtype=torch.float64) 0.6733732410807647
tensor(0.6740, dtype=torch.float64) 0.6682239459260506
tensor(0.6765, dtype=torch.float64) 0.6711175021269229
tensor(0.6777, dtype=torch.float64) 0.6728193416173774
tensor(0.6765, dtype=torch.float64) 0.6715307864861114
tensor(0.6777, dtype=torch.float64) 0.6734587829931238
tensor(0.6777, dtype=torch.float64) 0.6723586336640096
tensor(0.6765, dtype=torch.float64) 0.6702354763754423
tensor(0.6740, dtype=torch.float64) 0.6669804650870285
tensor(0.6802, dtype=torch.float64) 0.6742433863894752
tensor(0.6827, dtype=torch.float64) 0.6774450232883432
tensor(0.6802, dtype=torch.float64) 0.6751685647745043
tensor(0.6814, dtype=torch.float64) 0.677039812603796
tensor(0.6790, dtype=torch.float64) 0.6739718984070063
tensor(0.6753, dtype=torch.float64) 0.6704232800496812
tensor(0.6802, dtype=torch.float64) 0.6751892103787324
tensor(0.6728, dtype=torch.float64) 0.666779560392554
tensor(0.6704, dtype=torch.float64) 0.6629042033033944
tensor(0.6753, dtype=torch.float64) 0.6673752686562592
 77%|███████▋  | 231/300 [00:01<00:00, 207.14it/s]tensor(0.6753, dtype=torch.float64) 0.6670477086159311
tensor(0.6790, dtype=torch.float64) 0.6724399746495083
tensor(0.6777, dtype=torch.float64) 0.6724012834216451
tensor(0.6802, dtype=torch.float64) 0.6762287764814284
tensor(0.6888, dtype=torch.float64) 0.6861539763740718
tensor(0.6888, dtype=torch.float64) 0.6861825237656315
tensor(0.6851, dtype=torch.float64) 0.6818305191012078
tensor(0.6851, dtype=torch.float64) 0.6808426185220078
tensor(0.6740, dtype=torch.float64) 0.6685076862091799
tensor(0.6704, dtype=torch.float64) 0.6634943152930552
tensor(0.6740, dtype=torch.float64) 0.6665064559560973
tensor(0.6765, dtype=torch.float64) 0.6695667115521049
tensor(0.6740, dtype=torch.float64) 0.6685530215606608
tensor(0.6827, dtype=torch.float64) 0.6782320086213559
tensor(0.6839, dtype=torch.float64) 0.6794053564602888
tensor(0.6839, dtype=torch.float64) 0.6800118828605699
tensor(0.6851, dtype=torch.float64) 0.6807983603258027
tensor(0.6839, dtype=torch.float64) 0.6792772075812857
tensor(0.6814, dtype=torch.float64) 0.6763834844483935
tensor(0.6790, dtype=torch.float64) 0.6737358402222724
tensor(0.6790, dtype=torch.float64) 0.6726309783285913
tensor(0.6814, dtype=torch.float64) 0.674753489138454
 84%|████████▍ | 253/300 [00:01<00:00, 209.76it/s]tensor(0.6814, dtype=torch.float64) 0.6743148253652841
tensor(0.6814, dtype=torch.float64) 0.673915223011164
tensor(0.6827, dtype=torch.float64) 0.6761206293182584
tensor(0.6863, dtype=torch.float64) 0.6798613181536446
tensor(0.6839, dtype=torch.float64) 0.6783807201280341
tensor(0.6851, dtype=torch.float64) 0.6803733801733578
tensor(0.6802, dtype=torch.float64) 0.6755819189159682
tensor(0.6827, dtype=torch.float64) 0.6775129799041916
tensor(0.6827, dtype=torch.float64) 0.6764862455619048
tensor(0.6814, dtype=torch.float64) 0.6753321778016059
tensor(0.6802, dtype=torch.float64) 0.6749312619306986
tensor(0.6802, dtype=torch.float64) 0.6755908646447383
tensor(0.6790, dtype=torch.float64) 0.6751192013415361
tensor(0.6790, dtype=torch.float64) 0.6749023587717623
tensor(0.6851, dtype=torch.float64) 0.6814121134395438
tensor(0.6839, dtype=torch.float64) 0.6800132155124138
tensor(0.6777, dtype=torch.float64) 0.6735340208397893
tensor(0.6790, dtype=torch.float64) 0.6746269480174871
tensor(0.6777, dtype=torch.float64) 0.6733640793597419
tensor(0.6777, dtype=torch.float64) 0.6731374788909354
tensor(0.6777, dtype=torch.float64) 0.6735339046360269
tensor(0.6716, dtype=torch.float64) 0.6668424990306362
 92%|█████████▏| 275/300 [00:01<00:00, 211.63it/s]tensor(0.6728, dtype=torch.float64) 0.6673747610866302
tensor(0.6728, dtype=torch.float64) 0.6673509304679441
tensor(0.6728, dtype=torch.float64) 0.6663315459237118
tensor(0.6716, dtype=torch.float64) 0.6648197073366232
tensor(0.6802, dtype=torch.float64) 0.6744437781595529
tensor(0.6814, dtype=torch.float64) 0.675986898860589
tensor(0.6888, dtype=torch.float64) 0.6835278895849151
tensor(0.6925, dtype=torch.float64) 0.6878927231676171
tensor(0.6839, dtype=torch.float64) 0.6792604617939666
tensor(0.6790, dtype=torch.float64) 0.6737022785694128
tensor(0.6728, dtype=torch.float64) 0.6664238049367283
tensor(0.6704, dtype=torch.float64) 0.6630936644185786
tensor(0.6728, dtype=torch.float64) 0.6659709396395399
tensor(0.6777, dtype=torch.float64) 0.6718459106835141
tensor(0.6839, dtype=torch.float64) 0.6788726433333573
tensor(0.6876, dtype=torch.float64) 0.6824097873283675
tensor(0.6913, dtype=torch.float64) 0.6862591805549472
tensor(0.6876, dtype=torch.float64) 0.6827494431592787
tensor(0.6839, dtype=torch.float64) 0.6782629147846826
tensor(0.6753, dtype=torch.float64) 0.668958738523278
tensor(0.6777, dtype=torch.float64) 0.6712310000856918
tensor(0.6777, dtype=torch.float64) 0.671105746631756
 99%|█████████▉| 297/300 [00:01<00:00, 212.95it/s]tensor(0.6888, dtype=torch.float64) 0.6832106842564851
tensor(0.6937, dtype=torch.float64) 0.6881970389002182
tensor(0.6937, dtype=torch.float64) 0.6884700073302975
100%|██████████| 300/300 [00:01<00:00, 200.06it/s]
tran.0.fc1.weight Parameter containing:
tensor([[ 2.3175e-01, -2.1891e-03,  2.4781e-03,  ...,  4.8791e-03,
          7.1543e-03,  1.9679e-03],
        [-1.5231e-03,  3.5258e-01, -3.0332e-03,  ..., -7.3608e-04,
         -1.0887e-03, -2.9465e-03],
        [ 1.0299e-02, -2.1074e-03,  3.8044e-01,  ..., -3.9862e-03,
          1.3592e-02,  2.4914e-03],
        ...,
        [-2.9881e-04,  1.4296e-03, -1.1538e-03,  ...,  1.0917e-01,
          4.6337e-03,  4.2685e-04],
        [ 7.9241e-03,  1.1328e-03,  3.2793e-03,  ..., -1.3817e-03,
          2.1336e-01, -3.1196e-03],
        [ 1.1133e-02, -9.1606e-03, -2.8485e-03,  ..., -4.9071e-04,
         -1.0225e-02,  3.8946e-01]], device='cuda:0')
tran.0.fc1.bias Parameter containing:
tensor([ 1.4686e-03,  1.0384e-03,  1.5666e-03,  1.4216e-03,  5.0171e-03,
         7.3312e-04,  3.4172e-04,  1.3931e-04,  7.3123e-05,  1.4162e-03,
        -1.0683e-03, -3.2413e-04,  1.5136e-03, -3.6013e-04,  3.1098e-05,
        -3.2756e-05,  1.9168e-04,  2.0323e-04,  1.1843e-05,  8.8916e-03,
         1.1760e-03, -2.2755e-04, -1.2719e-04,  2.9627e-04, -6.3772e-04,
         1.1749e-03,  2.2433e-03,  8.6548e-04, -7.4593e-04, -2.4757e-04,
         7.2204e-04, -1.2913e-03,  9.3627e-04, -5.6042e-06, -7.7626e-05,
         6.0582e-05, -1.0716e-03, -1.8320e-05,  6.3491e-04,  4.1538e-03,
         1.1467e-03,  1.6748e-03,  1.0511e-04,  1.4301e-03,  2.3001e-03,
        -8.1362e-05,  4.7679e-04, -9.4479e-04, -9.6342e-04,  2.6630e-04,
         4.6611e-04,  8.1316e-04,  2.0397e-03,  8.7816e-04,  9.0255e-04,
         6.3234e-04,  1.4538e-03,  5.5064e-04,  2.0377e-03, -2.2306e-04,
        -3.2545e-04,  5.3139e-04, -1.1865e-03,  4.5718e-05,  1.2607e-03,
         2.0926e-03,  7.5208e-04,  1.8451e-03,  1.2639e-03,  1.7468e-03,
         9.5935e-04, -1.1135e-03,  2.6392e-04,  1.2159e-04,  9.3952e-04,
         1.1755e-03, -1.0235e-03, -2.6479e-04,  1.1398e-03,  8.4635e-04,
         4.3511e-05,  1.1108e-03, -1.2966e-04,  6.4089e-04, -9.1175e-04,
         5.4129e-04, -1.6609e-04,  9.1374e-04,  5.6828e-04,  1.3177e-03,
        -4.5737e-05,  2.5038e-04,  4.9030e-04,  1.7118e-03,  1.0734e-03,
         1.3774e-03,  4.3690e-04,  5.6457e-04,  1.7791e-04,  1.9515e-03,
         2.0411e-04,  3.0730e-04, -3.3145e-04,  5.4072e-04, -7.3984e-05,
         4.8851e-04,  7.1648e-04,  1.4587e-04,  3.3539e-06,  6.6786e-04,
        -7.6767e-04,  9.4196e-04,  3.1271e-04, -2.1632e-04,  1.1522e-03,
        -4.5885e-04, -1.4180e-03,  9.1911e-04,  7.8132e-04, -1.0733e-03,
         4.6829e-04,  4.4991e-04,  7.2896e-04,  4.4465e-05,  1.5617e-04,
         7.1111e-05,  5.2435e-04,  3.7563e-04,  1.0566e-03,  9.9436e-04,
         1.6990e-03,  7.0666e-04,  4.2088e-03,  1.1313e-03, -2.5441e-05,
         1.4900e-03,  1.8962e-03, -7.7493e-04, -8.7315e-06, -6.9887e-04,
         8.9992e-04,  3.9806e-04, -2.4763e-04, -1.3029e-03, -4.8299e-04,
        -5.6359e-05, -1.0502e-04,  4.1450e-04,  2.3623e-04,  5.3240e-04,
         2.5645e-04,  4.7920e-04,  4.6076e-04,  7.1340e-04, -1.0205e-03,
        -7.4154e-05,  5.2344e-04,  3.7954e-04,  2.7274e-04, -3.0918e-04,
         4.7986e-04, -1.2507e-04,  4.1559e-04,  1.2725e-03, -1.1623e-04,
         6.1478e-04, -3.0339e-05,  4.0126e-04,  8.5482e-04,  6.7729e-04,
        -3.1572e-04,  6.6374e-04,  7.7689e-04,  2.6522e-03, -6.2007e-04,
        -5.5548e-04, -1.9301e-04, -4.7208e-04, -1.6258e-04,  2.5285e-03,
         3.3456e-04,  8.9589e-05, -9.3551e-04, -4.6764e-04,  2.6058e-04,
         1.2307e-03, -1.8955e-04,  1.5008e-03,  2.2542e-04,  3.8787e-05,
         6.0444e-04,  1.2803e-03,  7.2624e-04, -2.5521e-04,  7.8974e-04,
         4.5264e-04, -3.8767e-04, -1.2212e-03,  8.8641e-04,  4.9740e-04,
        -2.4544e-04,  4.7784e-05,  2.0452e-04,  2.0503e-03,  8.7105e-04,
         1.4884e-03,  3.0437e-04, -8.2466e-04, -9.8369e-04, -9.5991e-04,
         1.3691e-03,  1.9385e-03, -3.3340e-04,  6.9879e-04,  1.4671e-03,
         2.1677e-04,  5.8089e-04,  3.7735e-04, -1.3199e-03,  5.6731e-04,
         5.3815e-04, -8.0916e-05,  5.4145e-04, -8.9984e-04,  9.0815e-04,
        -5.0115e-04,  2.8770e-04,  2.4691e-04,  1.3200e-03, -4.0731e-04,
         5.5532e-04, -7.1518e-04,  1.1952e-04,  2.3209e-04,  1.0280e-03,
        -3.6697e-04,  5.3117e-04,  1.0750e-04,  1.7313e-03, -2.7006e-04,
        -2.8242e-04,  1.3030e-04,  4.9187e-04, -2.0882e-04,  6.5015e-04,
         7.7760e-04, -9.9928e-04,  6.3936e-04, -7.8305e-04,  4.4443e-04,
         2.7058e-04,  8.4255e-04,  1.5896e-04,  6.7144e-04,  1.1496e-03,
         8.4057e-04,  2.8040e-04,  2.1758e-04,  4.9209e-04, -1.3657e-03,
         4.7704e-04,  5.5991e-04,  5.0453e-04,  5.2211e-04,  6.4045e-04,
        -3.6654e-04, -1.0099e-03, -7.9889e-05,  1.4764e-04,  1.1535e-03,
         9.1084e-04,  8.4466e-04, -3.1529e-04,  5.0674e-04,  6.3661e-04,
        -2.9904e-04,  1.4521e-03,  1.1581e-03, -2.0990e-04,  5.1170e-04,
        -5.7942e-04, -4.2082e-05,  8.0367e-04,  2.1363e-04,  1.2418e-03,
        -2.9726e-04, -4.7701e-04,  3.0920e-04, -1.2028e-03,  5.2466e-04,
         6.0179e-04,  2.2189e-04,  4.1797e-03, -1.3069e-03,  2.9722e-04,
         7.1088e-04,  9.6385e-04,  3.7109e-04, -4.2491e-04,  1.7216e-03,
         1.5671e-03,  1.2052e-04,  1.3660e-03,  2.0000e-05, -2.3315e-04,
         9.7639e-04,  6.0927e-04,  1.2599e-03,  8.9128e-04, -2.8557e-05,
         7.3435e-04, -3.2867e-04,  3.2617e-04, -2.6656e-04, -4.9873e-04,
         2.1182e-04,  5.8778e-04, -3.3330e-04, -7.2622e-04, -9.9966e-05,
         3.1376e-04, -2.1003e-04,  7.6077e-04, -2.3630e-04,  1.0187e-03,
        -2.4153e-04,  9.1406e-04,  4.4450e-04, -1.1175e-03,  5.0697e-04,
         1.4265e-03, -1.0160e-03, -2.5455e-04,  3.4896e-04, -5.9841e-04,
        -7.9409e-06,  1.5724e-03,  1.1711e-04,  1.2377e-03,  3.4548e-04,
         9.5796e-04,  1.0798e-03,  4.6764e-04, -9.5218e-05,  1.8981e-04,
         4.1045e-04, -1.0484e-04,  3.1747e-04,  9.1197e-06, -6.8468e-05,
        -3.6462e-04,  1.4733e-03,  2.1084e-04,  2.7510e-04,  8.2128e-05,
         1.2226e-04,  5.1897e-04,  3.2142e-04,  5.8221e-04,  1.2193e-03,
        -8.8840e-05, -4.7754e-05, -6.4361e-04,  4.1283e-04,  7.7434e-04,
         6.0237e-04, -1.9035e-04, -3.2893e-04,  2.1697e-03,  1.6877e-03,
         1.2375e-04,  9.6629e-04, -1.1831e-03,  5.2380e-04, -6.2292e-04,
        -1.6962e-04, -4.7854e-04,  5.1619e-04,  4.4616e-04,  4.3973e-04,
         3.7506e-04,  9.6412e-04, -1.2234e-03, -2.9186e-04,  3.3258e-05,
        -2.0085e-04,  9.4993e-06, -3.2704e-04,  7.0033e-04,  1.1849e-03,
         6.2000e-04,  5.5795e-04, -1.3955e-03,  5.3962e-04, -9.3893e-05,
         1.4886e-03,  1.2246e-04,  7.2515e-05,  2.9678e-03, -1.3998e-03,
        -2.3262e-04,  5.6832e-04,  9.6110e-04,  1.5203e-04,  1.2413e-03,
         7.8626e-04,  3.2381e-04,  6.9884e-04,  9.7753e-04,  4.5109e-04,
        -3.5816e-04,  1.2584e-03,  1.4069e-03, -1.1952e-03,  4.8323e-04,
         2.2032e-04,  4.0332e-04,  5.9300e-04,  1.4914e-04,  3.8461e-05,
         7.6187e-04, -9.7112e-04,  1.1053e-04,  6.2395e-04,  2.0833e-04,
         3.8792e-04,  9.6512e-04,  9.9887e-05, -2.7555e-05,  1.5352e-04,
         2.1307e-04,  2.0812e-04,  1.5447e-04, -3.6778e-04,  1.7666e-03,
         1.3532e-03,  4.5051e-04, -8.4667e-04, -7.4992e-04,  1.4937e-03,
         3.8467e-04,  2.0375e-03,  2.0240e-04,  6.7414e-04,  2.0818e-05,
        -3.9424e-04, -1.1137e-03,  4.1094e-04,  1.0886e-03, -1.0087e-03,
         1.5162e-03,  1.5427e-04, -1.2192e-03,  5.2358e-04, -9.7911e-04,
         6.5455e-04,  5.3499e-04,  6.8614e-04,  6.0311e-04,  4.0725e-04,
         2.6681e-04,  1.2960e-03,  2.1546e-04, -1.8375e-04,  1.2007e-03,
         7.4043e-04,  3.5320e-04,  5.3066e-04, -2.2338e-06,  1.4002e-03,
         5.9184e-04,  2.2581e-04,  4.4369e-04, -3.5726e-04,  1.5703e-03,
         1.9121e-03, -1.8165e-04, -3.9786e-04,  6.8506e-04,  7.5025e-04,
         6.1036e-04, -6.7244e-04,  1.1899e-04,  2.7231e-04,  2.7695e-04,
         1.0389e-03,  1.7602e-03,  3.8082e-04,  5.9301e-04,  2.3980e-04,
         3.3889e-04,  7.1022e-05, -2.4726e-04,  9.2319e-05,  1.3047e-04,
         1.5773e-03,  1.9487e-04, -1.0256e-03, -1.5844e-04,  8.7948e-04,
         5.6056e-04, -3.4068e-04, -6.6887e-04, -2.5372e-05,  2.8877e-04,
         5.3332e-04, -1.8079e-04,  5.8080e-03,  1.9286e-05,  2.6296e-03,
         1.4539e-03,  4.4599e-04,  4.0879e-04,  1.0768e-03,  1.6499e-03,
         5.1827e-04,  1.5406e-03,  1.1296e-03, -5.6768e-04,  9.3593e-04,
         2.2584e-04,  1.1906e-03,  2.3162e-04,  7.5284e-04,  7.2958e-04,
         5.1420e-05, -7.5980e-04,  7.3966e-04,  4.8912e-04,  2.6367e-04,
         2.8408e-04,  7.9637e-04, -3.4796e-04,  7.2753e-04,  5.6912e-04,
         2.0409e-04,  1.4721e-03,  9.7127e-04, -8.1886e-04,  1.0950e-03,
         3.1741e-03, -3.1275e-04,  1.8847e-04, -4.8162e-04, -5.6377e-04,
         4.1246e-04, -8.8746e-05,  3.2157e-04,  1.8025e-03, -3.6427e-05,
         7.5133e-04,  9.3220e-04,  8.8214e-04,  5.3776e-04, -3.5830e-04,
         2.1770e-04,  1.5866e-04,  7.8324e-04,  1.8025e-04,  2.9000e-04,
         1.0754e-03,  3.8055e-04,  3.9746e-04,  8.6271e-05,  8.3214e-04,
        -8.4686e-05,  3.7643e-04, -5.0700e-04, -1.0324e-03, -2.1694e-04,
         5.4676e-04,  1.7325e-04,  9.9407e-05, -7.3342e-04,  7.4296e-04,
         1.4985e-03,  1.2881e-03,  8.9103e-05, -1.3006e-04,  4.8432e-04,
        -8.7770e-07,  1.9702e-03, -3.8753e-04,  9.3044e-04, -1.7751e-04,
        -1.4776e-04,  1.0192e-03, -4.0328e-04, -4.4234e-04, -2.5175e-04,
        -2.0586e-04,  2.5415e-04,  1.4408e-03,  1.1200e-03,  9.7936e-04,
         3.4859e-04, -6.3311e-05, -4.7617e-04,  3.6521e-04, -3.1937e-05,
         1.4090e-03,  1.3631e-03, -1.0928e-03,  5.3982e-04,  5.3951e-04,
        -5.9735e-04, -2.8484e-04, -3.2652e-04,  1.5617e-04,  6.7724e-05,
         1.4815e-03, -2.7161e-04,  1.0961e-04, -1.0584e-03,  2.0226e-04,
         7.5945e-04,  3.9502e-04, -1.4704e-04,  2.0008e-03,  2.6107e-03,
        -1.8537e-04,  1.5517e-03, -6.6083e-05,  8.1413e-04,  1.4774e-03,
         2.1149e-03,  4.4783e-05, -2.0412e-04,  8.6098e-04,  3.5989e-04,
         3.4538e-04,  6.4581e-04, -1.3697e-03, -4.2745e-04,  6.9897e-04,
        -1.3376e-03, -1.8686e-04, -7.1170e-05, -5.7569e-04,  2.7024e-04,
         7.1947e-04, -5.3872e-04, -1.4521e-04,  1.4843e-03,  1.2913e-03,
         1.5400e-04,  7.3898e-04,  2.5137e-03,  2.9154e-03,  1.6070e-03,
         9.1274e-05,  7.9322e-04, -2.5464e-04,  4.2247e-04,  4.5908e-04,
         1.2505e-03, -7.1231e-04,  1.5881e-04, -6.1221e-04,  4.5450e-04,
        -1.0592e-03, -1.4382e-04, -8.2429e-04,  9.8569e-04,  7.1447e-04,
         1.6431e-03,  5.2497e-04, -1.0582e-03,  3.5866e-04, -1.3791e-04,
         5.7318e-04,  6.8250e-04,  8.5184e-04,  5.4457e-04,  7.5039e-04,
         1.2222e-03, -1.0845e-03,  1.8720e-04, -5.3880e-04, -4.3020e-04,
         9.0717e-05,  3.6480e-04,  3.8720e-04,  1.7468e-05,  2.2821e-04,
         9.0729e-04,  6.6915e-04,  2.6112e-04,  1.7391e-04,  2.6365e-04,
         2.7087e-04, -1.1989e-03, -3.5089e-04,  6.7556e-04,  3.9800e-04,
         1.2142e-05,  1.7629e-03,  2.2199e-03,  1.1450e-03,  2.0223e-03,
         1.1091e-03,  1.7675e-03,  8.4608e-04,  1.1165e-03, -1.8659e-04,
         5.9750e-04,  5.8127e-04, -2.5468e-04,  1.1664e-04,  3.1626e-04,
         5.3220e-04,  3.8729e-04,  2.5426e-04,  9.1467e-04, -5.5336e-04,
         1.5329e-03], device='cuda:0')
tran.1.fc1.weight Parameter containing:
tensor([[ 9.5440e-02, -6.3586e-04,  1.2578e-03,  ...,  9.8218e-04,
          1.3487e-03, -2.0061e-03],
        [ 5.0054e-04,  2.0085e-01, -1.1010e-04,  ...,  1.1533e-03,
         -7.9314e-04, -1.1797e-03],
        [ 6.3466e-04,  7.9046e-04,  3.3457e-01,  ..., -1.3742e-03,
          3.7581e-03,  1.5813e-03],
        ...,
        [-6.0817e-04,  1.5918e-03,  4.6811e-03,  ...,  1.8636e-01,
          1.1820e-02, -5.3796e-03],
        [-1.8628e-03, -2.0489e-03,  6.6111e-03,  ...,  1.1335e-02,
          3.7299e-01,  1.6810e-03],
        [-8.5329e-04, -2.5014e-03, -1.1379e-04,  ..., -8.0479e-04,
          5.1868e-03,  2.0205e-01]], device='cuda:0')
tran.1.fc1.bias Parameter containing:
tensor([-9.4642e-05, -5.7342e-04,  1.3135e-03,  2.3406e-05,  9.9739e-04,
         7.3249e-04,  8.3584e-05,  2.9654e-03,  3.2276e-03,  1.5742e-04,
         1.0573e-03,  7.8386e-04, -5.0129e-04,  2.4853e-03,  1.0644e-03,
        -6.1774e-04,  7.8225e-04, -1.3410e-04, -1.4072e-03,  2.0491e-03,
         1.1829e-03, -2.4943e-04,  3.0830e-04,  7.4604e-04,  9.6352e-04,
        -1.0357e-03, -1.2452e-03,  9.7879e-04,  1.1445e-03,  1.2906e-03,
        -3.5527e-04,  1.7764e-04,  5.1405e-04,  4.0817e-04, -2.3481e-04,
        -1.2328e-04,  5.3518e-04,  6.5350e-04,  2.9798e-03,  1.3099e-03,
         6.8380e-04, -4.6793e-04,  2.2003e-03,  1.4451e-04,  1.0831e-03,
        -4.2094e-04,  5.1813e-04,  1.7591e-03,  6.5384e-04,  2.5928e-04,
         6.1884e-07,  2.6530e-04, -1.3787e-04,  1.6449e-04,  5.8644e-04,
        -9.2325e-06,  1.1510e-03,  1.0541e-04,  7.9027e-03,  5.9237e-04,
         1.1182e-03,  1.1215e-03, -3.5319e-06,  6.8783e-04,  4.1836e-04,
        -2.3198e-04,  6.7646e-04,  7.6870e-04, -1.8022e-04, -3.0593e-04,
         4.5974e-04, -3.1838e-04, -3.1314e-04, -3.8863e-05,  1.8526e-04,
         7.8718e-05,  1.7749e-04, -9.0665e-05,  7.4699e-04,  1.7912e-03,
        -8.6620e-04,  1.1675e-03,  5.3725e-04,  1.1720e-03, -8.5399e-04,
         6.6196e-04,  4.0550e-04,  5.5029e-04,  1.3373e-03,  1.0411e-04,
         8.0334e-04,  1.5984e-04,  2.3371e-03,  8.7699e-04,  1.4860e-03,
         4.5216e-04,  1.2671e-03,  7.8877e-04,  8.0125e-04, -4.3040e-04,
         1.2459e-03,  1.5932e-03, -1.1223e-03, -3.5935e-05,  2.6383e-03,
         1.0698e-03,  3.0457e-05,  1.1614e-04,  4.1067e-04, -4.3966e-04,
         7.1851e-04,  8.6685e-04,  2.3842e-03,  8.3207e-04,  3.8474e-05,
        -9.3121e-04,  4.2400e-04,  9.4334e-04,  1.5381e-03, -3.0954e-04,
         4.6318e-04,  6.0915e-04,  1.8646e-04, -9.8074e-04, -3.4520e-05,
        -7.8356e-05,  8.9340e-04,  2.2270e-04,  5.6977e-04, -8.7126e-04,
        -3.0846e-04,  2.6357e-04,  3.8508e-04,  1.4812e-04,  7.7548e-04,
         3.0458e-04,  9.7751e-04,  1.6733e-04,  1.3143e-03, -5.1003e-04,
        -1.7096e-04,  8.5910e-04, -1.0313e-04,  1.4643e-03, -8.9439e-05,
         2.2369e-04, -1.7856e-04,  3.9251e-05,  9.6375e-04,  8.8630e-04,
         5.7808e-04,  4.4881e-04,  2.4844e-04,  2.0622e-04,  3.4147e-04,
         7.6204e-04,  5.6008e-04,  8.6933e-04,  1.8800e-03,  8.2111e-04,
         6.9684e-04,  6.3734e-04,  3.9825e-04,  1.0683e-03,  1.2339e-04,
         2.6327e-04,  5.5492e-04,  2.6688e-04, -2.0988e-03,  1.1174e-04,
        -2.1732e-04,  4.7703e-04,  4.3557e-04,  2.0763e-04,  9.1349e-04,
         4.1746e-04,  5.3285e-04, -1.9796e-03,  1.3863e-03,  4.0803e-04,
        -5.4790e-06,  1.8226e-03, -2.7489e-04,  3.7988e-04,  9.9645e-06,
        -5.6414e-05,  3.1109e-04,  1.0327e-03,  4.5325e-05,  7.3674e-04,
        -8.8869e-04,  5.2356e-04,  1.8427e-05, -8.0674e-04, -2.2985e-05,
         4.9995e-03,  5.7666e-04,  5.0767e-04,  9.9001e-04,  1.5681e-03,
         3.9010e-04, -1.2190e-04,  7.0628e-04,  1.0389e-03,  8.4758e-04,
         5.1827e-04,  7.8636e-04,  2.9830e-04,  5.7595e-04,  6.8950e-04,
         7.0322e-04, -2.7153e-04,  4.0563e-04,  8.7388e-04,  1.5994e-04,
         3.5216e-03,  1.1716e-03, -5.5888e-04, -3.7075e-04,  1.7215e-03,
         1.6605e-03,  1.3972e-03, -7.2048e-04,  2.2890e-04,  4.7684e-04,
         2.9842e-05, -2.7826e-04, -9.0932e-05,  1.1858e-03,  9.6444e-04,
        -2.1354e-04,  4.0023e-04, -4.9737e-04, -1.1086e-04, -6.0334e-05,
         1.2210e-03, -8.4126e-05,  1.8899e-03,  7.1932e-06,  4.1300e-04,
         8.4352e-04, -1.4116e-03,  5.5660e-04, -6.1365e-04,  1.7662e-04,
         2.7194e-04,  3.1228e-04,  5.6925e-04,  3.4099e-04,  3.9446e-04,
         8.6178e-05,  5.0035e-04,  6.5336e-04,  1.9549e-03, -6.6670e-04,
         9.2096e-04,  3.7628e-04, -5.1115e-04,  1.2369e-03,  6.6206e-04,
         1.0131e-04, -9.4844e-04,  1.0816e-03,  1.2244e-03, -2.0482e-04,
        -1.4164e-03, -3.1663e-04, -1.6011e-04,  8.5900e-04,  8.8389e-04,
         1.5706e-03,  4.0416e-05,  2.0031e-03,  5.7924e-05, -7.2025e-04,
        -5.0156e-04,  5.2009e-04,  1.0942e-03,  8.1993e-04,  6.3874e-04,
        -1.4400e-04,  2.9888e-04,  2.4166e-04,  5.0479e-04,  3.8771e-04,
         3.3892e-04,  5.9697e-04,  1.1023e-03, -6.2334e-04, -3.3751e-04,
        -8.4447e-05,  4.7582e-05, -9.0417e-06,  1.5239e-04,  6.8001e-04,
         7.9385e-04, -1.3236e-04,  1.4496e-03,  6.6296e-04, -6.0340e-04,
         7.7100e-04,  1.1593e-03,  3.1274e-04,  1.1793e-03,  3.1241e-04,
        -8.3908e-04,  3.9966e-04,  2.4794e-04,  7.1371e-04,  1.2293e-03,
         6.0992e-04,  1.7500e-03,  5.7325e-04, -2.4198e-04, -4.8000e-04,
         2.1774e-04, -3.5594e-04,  4.7661e-04,  7.3972e-04,  1.2360e-03,
        -6.5665e-05, -1.9323e-03,  6.6972e-04, -8.5615e-04,  1.1356e-03,
         5.5344e-04,  1.0268e-03,  1.0321e-03,  2.0891e-04, -7.4096e-04,
         7.9561e-04, -8.7333e-04, -1.6034e-04,  2.4272e-03,  4.2505e-04,
         3.2423e-04,  1.2239e-03, -6.3748e-05,  1.2379e-03, -3.0010e-04,
         3.3431e-04,  4.2449e-04,  2.7550e-04, -5.3357e-04,  1.6570e-03,
         2.9998e-04,  5.7730e-04,  8.2472e-04,  3.2673e-05,  5.8578e-04,
         1.1581e-03, -2.3603e-04, -1.6489e-05,  1.1025e-03,  3.6426e-04,
         4.0023e-03,  2.9159e-04,  1.2605e-04,  3.1334e-04,  5.1772e-03,
         5.0472e-04,  1.0924e-03, -4.9665e-04,  1.6704e-03,  3.6013e-04,
        -1.4298e-03,  1.9243e-04,  9.2138e-04,  4.5190e-04, -6.9056e-05,
         5.6987e-04,  8.4194e-04,  5.4639e-04,  1.6606e-04,  3.4995e-05,
        -4.6687e-05,  1.1900e-03,  6.0721e-04,  1.7031e-03, -7.8960e-04,
         1.6036e-03,  2.5068e-03, -4.7273e-04,  9.7504e-06,  1.6862e-04,
        -1.7009e-04,  7.3924e-04,  9.7382e-04, -1.0652e-05,  2.6877e-04,
        -3.6225e-04,  6.3263e-04,  7.8071e-04,  3.2549e-04, -4.8957e-05,
        -7.2563e-04,  2.0571e-05,  4.1587e-04,  1.2532e-03, -5.9857e-04,
         6.8882e-04,  1.4480e-03,  9.5767e-04, -1.7777e-03,  1.8745e-03,
         5.2156e-04, -4.7231e-04,  1.8986e-03,  2.5639e-04,  4.2855e-04,
        -4.9375e-05,  6.0820e-04, -6.8429e-04,  2.3050e-05,  9.6114e-05,
         2.8143e-03,  4.5313e-03, -1.9692e-03,  2.9372e-04, -3.5588e-04,
         2.1755e-04,  9.3530e-04,  3.9296e-03,  8.8700e-04,  8.9094e-04,
         1.6003e-04,  7.5637e-04,  1.9251e-03,  3.7948e-03, -1.4089e-03,
        -1.6588e-06,  5.5751e-04,  1.3254e-04,  4.7561e-03,  1.1164e-04,
         2.1121e-03,  1.4127e-03, -7.8645e-04, -7.7268e-04,  9.5416e-04,
         5.2238e-04,  7.4019e-04,  2.6240e-04,  2.1477e-03,  1.7709e-03,
        -4.7319e-05,  1.7041e-03, -7.3048e-04,  5.6018e-04, -2.0550e-04,
         6.2001e-04,  2.7893e-04,  5.7676e-04, -4.4108e-04,  2.8439e-03,
         3.3700e-03, -5.2875e-04,  1.1141e-03,  3.6833e-03,  1.1398e-03,
         6.2314e-04,  1.3525e-02,  2.9391e-04, -5.9296e-04,  1.3693e-03,
        -6.1269e-04,  1.5367e-04,  6.4526e-04,  1.1907e-03,  6.2764e-04,
         3.8294e-04,  1.0460e-03,  1.5086e-04, -7.0922e-04, -7.3161e-04,
         1.5906e-03,  4.9700e-04,  9.6222e-04,  9.3254e-04,  3.3780e-04,
        -1.7250e-03,  4.8517e-04,  3.3218e-03, -5.1543e-04, -4.3321e-04,
         5.2635e-04,  3.0963e-04,  2.4812e-03,  8.2634e-04,  8.5730e-04,
         6.3090e-04,  6.9555e-04, -1.0001e-03,  1.1211e-02,  1.6326e-03,
         1.0426e-03,  1.6447e-03,  7.7760e-04,  4.5327e-04,  1.3345e-03,
         1.1552e-04, -8.4226e-05,  3.8723e-04,  9.2693e-04,  3.9119e-04,
         1.2863e-04,  1.2079e-03,  5.8205e-04,  8.2929e-04,  1.3379e-05,
         6.7021e-04, -5.5399e-04,  4.2485e-05,  1.4277e-03,  2.1264e-03,
         5.6425e-04,  1.3307e-04,  8.4864e-04,  9.2388e-04,  4.1379e-04,
         8.2317e-04, -2.2849e-06,  8.8514e-05, -8.3545e-04,  9.3811e-04,
         9.5556e-04, -3.5820e-04,  1.1072e-04, -1.2260e-03, -6.8124e-05,
         8.9061e-04,  2.3653e-03,  2.4977e-05,  3.6192e-03,  1.4429e-03,
        -6.9530e-05,  3.0048e-04,  1.9758e-03,  3.9622e-03,  3.6921e-04,
         3.5182e-04,  1.6648e-03, -3.7271e-04, -1.3220e-03,  5.0913e-04,
         4.1099e-04, -1.2727e-03,  1.5269e-02,  6.7012e-04, -4.0817e-04,
         1.1255e-03,  1.5129e-03,  1.1956e-03, -8.7113e-04, -6.0331e-04,
         2.9495e-04,  5.8075e-04,  1.1567e-03,  3.1249e-03, -8.3502e-04,
         2.4567e-04,  9.2347e-04,  6.1688e-04,  2.9418e-04,  1.2259e-04,
         6.7207e-04,  2.8393e-04,  5.1359e-04,  5.0415e-04,  1.1635e-03,
        -1.6861e-04, -3.9473e-04,  2.7652e-04, -7.2206e-04,  1.2168e-03,
         5.5902e-04,  8.7945e-04,  6.2070e-05, -1.0513e-03,  1.0104e-03,
         6.1304e-04, -1.2722e-04,  1.2153e-04,  1.5722e-03, -9.2386e-04,
         3.9161e-03,  2.1900e-03, -1.1966e-05,  4.2462e-04,  1.2141e-03,
         1.4617e-03,  9.6181e-04, -2.0679e-04,  8.6877e-04,  3.9484e-04,
         6.7531e-04,  1.4080e-03,  5.7137e-04,  8.2441e-04,  1.5638e-04,
         4.0768e-04,  3.4208e-05,  8.5704e-04, -3.8445e-04,  4.7757e-04,
        -2.3205e-04,  6.2656e-04, -6.8483e-04, -4.7893e-04, -5.3345e-04,
        -4.2257e-04, -2.9640e-04,  9.8648e-03,  2.7338e-04,  1.8026e-03,
         8.5194e-04,  2.6903e-03,  7.5792e-04,  1.4267e-03,  2.9895e-04,
         2.3595e-03,  2.6531e-04,  7.5869e-04, -1.8287e-04,  6.9552e-04,
         2.5667e-04,  8.8562e-06, -4.5537e-04, -7.7546e-04,  7.8933e-04,
         3.2720e-04, -3.5855e-04,  4.1939e-03,  1.4193e-04,  5.0018e-04,
         1.1707e-03,  3.0358e-03,  3.0333e-03,  6.1670e-04,  4.5129e-04,
         8.0604e-04,  5.0635e-04,  9.0456e-04,  1.5379e-03,  9.5453e-04,
        -1.9698e-04, -5.4996e-04, -1.5606e-04,  3.5234e-04,  1.9702e-03,
         4.3377e-04,  2.6695e-04,  3.2421e-05,  3.1924e-04, -2.3952e-04,
         1.8279e-04, -6.0452e-04,  4.2579e-04,  8.5845e-05,  9.8678e-04,
         7.9332e-04,  2.9517e-05,  2.4504e-04,  5.0445e-04,  6.4339e-04,
         5.7242e-03, -1.0612e-04, -6.8429e-04,  1.5116e-03,  4.6449e-04,
         1.0733e-03,  1.4917e-03,  8.0596e-05,  2.0688e-03,  3.0538e-04,
         4.2589e-04,  1.1959e-03,  1.9984e-03,  1.4770e-03,  6.8792e-04,
        -2.2796e-05,  1.9775e-03, -7.8500e-04,  3.8805e-04, -1.3931e-04,
        -7.2562e-04,  5.4707e-04, -7.2465e-04,  3.3027e-05,  3.5062e-04,
         3.2232e-04,  3.7056e-04,  2.7529e-04,  6.4655e-04, -2.6671e-03,
         9.4874e-04,  1.6211e-03,  9.6669e-04,  4.2632e-04,  1.1771e-03,
        -4.4547e-04, -2.3208e-03,  8.1807e-04,  6.3513e-04,  3.4257e-04,
         1.1525e-04,  9.0842e-04,  1.7192e-03,  4.6374e-04,  1.0870e-03,
         2.8165e-04,  6.2313e-05, -2.0659e-04,  3.0823e-04, -9.8748e-04,
         1.3437e-03,  5.5249e-05], device='cuda:0')
classifier.fc1.weight Parameter containing:
tensor([[-0.0035, -0.0151, -0.0263,  ...,  0.0527, -0.0115,  0.0403],
        [-0.0283, -0.0062,  0.0357,  ...,  0.0413,  0.0092, -0.0088],
        [ 0.0376, -0.0015,  0.0040,  ...,  0.0100, -0.0007, -0.0109],
        ...,
        [ 0.0011,  0.0623, -0.0439,  ...,  0.0265,  0.0387, -0.0014],
        [-0.0252, -0.0593, -0.0029,  ...,  0.0145,  0.0979,  0.0250],
        [ 0.0080,  0.0441,  0.0294,  ..., -0.0204,  0.0045, -0.0221]],
       device='cuda:0', requires_grad=True)
classifier.fc1.bias Parameter containing:
tensor([ 6.2822e-03, -5.0509e-03,  1.5643e-02,  1.0004e-02, -1.0329e-02,
        -1.4616e-02,  8.3426e-03, -1.6802e-02,  1.5342e-02,  1.4028e-02,
         1.5239e-01,  3.8798e-04, -1.6169e-03, -4.1470e-03, -1.7070e-02,
        -3.5245e-03, -2.9639e-02, -1.1793e-02, -7.2271e-02,  7.8289e-03,
        -1.3469e-02, -4.6524e-03, -1.2719e-02,  8.8716e-04, -7.2265e-03,
         6.3894e-03, -1.4847e-02, -1.0636e-02,  3.9580e-03, -3.4291e-03,
        -7.9973e-04, -3.7387e-02, -7.9560e-04,  9.4557e-03, -3.6135e-02,
         7.7019e-05, -4.3825e-04,  1.8413e-02,  4.9772e-03,  1.2487e-02,
        -1.3572e-02, -1.3546e-02, -1.1489e-02,  1.9781e-03,  2.1283e-03,
        -4.6855e-03, -1.1885e-02, -6.9930e-03, -1.5743e-02, -6.7454e-03,
         3.0588e-02, -1.6700e-02, -1.4367e-02,  4.4431e-03, -1.7573e-02,
         2.1776e-02,  1.5571e-02, -8.0975e-02,  5.1359e-03,  6.5387e-02,
         1.9329e-03,  7.7050e-03, -1.3187e-02, -1.3630e-01, -3.7670e-03,
         1.0263e-02, -1.2108e-02,  1.2912e-03, -3.2682e-03, -1.5017e-02,
         1.4291e-02, -1.0628e-01,  1.1584e-02,  2.5365e-03, -5.6382e-03,
         1.4086e-02,  9.6680e-03, -1.6763e-03,  5.5159e-03,  1.8504e-02,
         5.6848e-04,  1.5769e-02, -5.0784e-03,  2.8881e-02,  1.0115e-02,
         1.9251e-02,  6.9890e-04, -1.9594e-02, -2.9364e-02,  1.4861e-02,
         3.7051e-03, -4.0762e-02, -7.9510e-03, -4.5392e-03,  8.7480e-03,
         2.0596e-02,  7.2920e-03,  1.5056e-02,  1.9112e-02,  1.4028e-02,
        -7.4293e-03, -3.9519e-02, -1.2678e-03,  8.3371e-03,  1.0280e-02,
        -1.4897e-02, -2.5790e-03,  1.6653e-02, -1.3831e-02, -8.1116e-03,
         1.0615e-02, -9.0451e-03, -8.3570e-03,  5.9532e-03,  1.6400e-02,
         8.4306e-03, -1.9331e-03, -1.1249e-02,  1.4113e-01,  1.2382e-03,
        -1.8366e-02, -1.4735e-02,  2.5282e-03,  4.2449e-03,  9.1714e-03,
        -2.1610e-04, -1.6049e-02,  1.0523e-02, -9.1133e-05,  3.3547e-02,
         9.9357e-03,  7.4689e-03,  1.2559e-02,  2.6749e-03,  3.7166e-02,
         7.5727e-02,  1.1841e-02, -1.8372e-02, -3.3723e-03, -2.0428e-03,
        -1.8171e-02, -8.0692e-03,  4.2424e-03,  1.6691e-02,  1.2849e-02,
         1.6467e-02, -1.1710e-02, -7.7110e-03, -1.3136e-02, -8.6943e-03,
         3.8801e-03,  9.3993e-02,  1.0622e-02,  1.1911e-02, -4.0995e-03,
        -3.7586e-03,  1.2418e-02,  3.6903e-02, -2.0346e-02,  1.2724e-03,
        -1.1638e-02,  2.8134e-03, -6.4570e-03, -2.0058e-02,  8.0818e-03,
         8.5469e-03,  1.2103e-02,  4.6847e-03, -7.0021e-03,  4.4416e-03,
        -1.6641e-03,  4.2391e-03,  1.4667e-02, -6.1385e-03,  5.3723e-03,
         5.5509e-03, -1.1415e-02,  1.0833e-02, -1.7484e-03, -3.4613e-03,
         1.3965e-03, -8.6431e-03,  4.3499e-04, -7.6893e-03, -1.0063e-02,
        -1.2592e-02,  2.7632e-02, -8.0821e-04,  9.8557e-04,  1.5209e-02,
        -1.0162e-02,  5.7929e-03, -8.8522e-02, -1.2934e-02, -3.4454e-04,
         7.5996e-03,  1.8259e-01,  2.7490e-03, -4.9610e-03, -1.7650e-03,
        -7.8923e-03,  1.5063e-02,  4.0566e-03, -2.5319e-03, -3.0701e-02,
         1.7154e-02,  7.6651e-03,  1.2235e-02, -5.5536e-03, -7.2108e-03,
         1.2071e-02,  2.7655e-03, -3.0307e-04,  7.1173e-03, -1.2807e-02,
         9.1648e-03,  1.2858e-02,  4.2098e-03,  4.9706e-03,  7.3315e-04,
        -2.3409e-03, -3.0150e-03, -3.4430e-03, -2.0061e-02, -7.9826e-03,
        -1.3953e-02, -1.0970e-02, -1.9423e-02, -4.3335e-02, -1.4591e-02,
        -1.7362e-02,  1.3376e-01, -1.2498e-02, -1.7574e-02,  1.8404e-01,
         8.0225e-03, -9.6821e-03,  3.5764e-03,  1.5027e-03, -1.1278e-02,
        -2.0500e-02, -4.5146e-02, -3.2781e-03, -7.6191e-02, -6.1042e-03,
        -9.7914e-03, -4.1343e-03, -2.4345e-02, -2.4718e-03,  2.0981e-03,
         2.3011e-02, -7.3632e-03, -1.2817e-02,  4.8202e-03, -4.5766e-03,
        -3.8774e-03], device='cuda:0', requires_grad=True)
classifier.fc2.weight Parameter containing:
tensor([[ 0.0578,  0.0912, -0.0184,  ...,  0.0672,  0.0822, -0.0706],
        [-0.0691,  0.0182, -0.0836,  ...,  0.0105, -0.0655, -0.0318],
        [-0.1215, -0.1475,  0.0313,  ..., -0.0919, -0.0367, -0.1155],
        ...,
        [ 0.0139,  0.0094,  0.0073,  ..., -0.0514,  0.0562,  0.0081],
        [-0.0183, -0.0669, -0.0607,  ...,  0.0803, -0.0551, -0.1093],
        [ 0.0052, -0.0666, -0.0715,  ...,  0.0806, -0.0230, -0.0732]],
       device='cuda:0', requires_grad=True)
classifier.fc2.bias Parameter containing:
tensor([ 0.0119, -0.1204, -0.0066,  0.0937, -0.0186,  0.0100, -0.0415],
       device='cuda:0', requires_grad=True)
classifier.layernorm.weight Parameter containing:
tensor([0.7027, 0.6527, 0.7874, 0.7765, 0.4079, 0.7883, 0.7881, 0.5071, 0.5991,
        0.5722, 0.6212, 0.7056, 0.7450, 0.7008, 0.6102, 0.7345, 0.7775, 0.7973,
        0.6139, 0.7768, 0.7440, 0.7215, 0.7415, 0.5295, 0.6847, 0.6264, 0.6607,
        0.6711, 0.7379, 0.6123, 0.7948, 0.7036, 0.7785, 0.7534, 0.6874, 0.7112,
        0.6581, 0.7726, 0.7354, 0.7589, 0.6158, 0.6571, 0.7788, 0.6505, 0.7056,
        0.7329, 0.6549, 0.7493, 0.6796, 0.7161, 0.7409, 0.6664, 0.7431, 0.6906,
        0.7193, 0.7728, 0.7580, 0.6814, 0.7674, 0.7139, 0.8425, 0.7166, 0.6932,
        0.5062, 0.6027, 0.7163, 0.7384, 0.7485, 0.7656, 0.5760, 0.7586, 0.5536,
        0.7142, 0.7506, 0.7620, 0.4938, 0.5682, 0.8063, 0.5778, 0.6697, 0.7179,
        0.7125, 0.8351, 0.6749, 0.6640, 0.7096, 0.7838, 0.7395, 0.7174, 0.7376,
        0.7460, 0.7811, 0.7565, 0.5105, 0.8201, 0.7093, 0.7215, 0.8247, 0.7148,
        0.6584, 0.7355, 0.6994, 0.5903, 0.7952, 0.7997, 0.7006, 0.7113, 0.7917,
        0.8028, 0.5962, 0.7380, 0.7397, 0.7667, 0.7223, 0.8300, 0.5959, 0.6406,
        0.7665, 0.6085, 0.7817, 0.6613, 0.6517, 0.7768, 0.5916, 0.5956, 0.8251,
        0.8020, 0.7586, 0.6500, 0.7081, 0.7440, 0.6636, 0.5562, 0.7540, 0.7349,
        0.7327, 0.5589, 0.6902, 0.7351, 0.7171, 0.8203, 0.6002, 0.6546, 0.7097,
        0.6247, 0.8133, 0.6914, 0.8121, 0.7473, 0.7950, 0.6938, 0.6167, 0.6988,
        0.7578, 0.7242, 0.7027, 0.6930, 0.6067, 0.7701, 0.8388, 0.7062, 0.7651,
        0.7417, 0.6790, 0.7327, 0.8022, 0.6179, 0.8385, 0.7326, 0.6465, 0.6066,
        0.6349, 0.5614, 0.8120, 0.7126, 0.7070, 0.5684, 0.7660, 0.6172, 0.7397,
        0.7762, 0.6959, 0.7389, 0.7281, 0.5511, 0.7865, 0.7737, 0.6070, 0.5452,
        0.6178, 0.7557, 0.5728, 0.6154, 0.7869, 0.7344, 0.3731, 0.4963, 0.7371,
        0.4949, 0.7408, 0.7733, 0.6789, 0.7765, 0.6809, 0.7165, 0.6491, 0.7492,
        0.7028, 0.7174, 0.6554, 0.7055, 0.6306, 0.8270, 0.6554, 0.6291, 0.8172,
        0.7123, 0.6259, 0.8036, 0.7014, 0.6359, 0.7309, 0.7705, 0.7520, 0.7613,
        0.7791, 0.6843, 0.6887, 0.6152, 0.7163, 0.7677, 0.5993, 0.5241, 0.7044,
        0.5526, 0.7769, 0.7557, 0.7126, 0.7981, 0.5562, 0.7498, 0.5675, 0.7853,
        0.6611, 0.6150, 0.6794, 0.6799, 0.7530, 0.7092, 0.5979, 0.7253, 0.5772,
        0.6922, 0.6763, 0.7910, 0.6455], device='cuda:0', requires_grad=True)
classifier.layernorm.bias Parameter containing:
tensor([-1.5964e-02,  3.3901e-02, -9.1735e-02, -1.5719e-02,  3.9543e-02,
         4.5235e-02, -6.6788e-02,  3.2548e-02,  4.8147e-02,  2.7676e-02,
        -1.4213e-01, -1.9804e-02,  1.2721e-01,  9.0731e-02,  4.2415e-02,
         1.1084e-01,  1.0121e-01,  6.6630e-02,  1.5936e-01,  5.1605e-03,
         8.2367e-02,  3.5740e-02,  1.1663e-01, -3.2752e-02,  1.4110e-01,
        -2.4611e-02,  7.2280e-02,  5.0908e-02,  1.0045e-01,  1.3107e-02,
         9.8182e-02,  1.4353e-01,  4.1009e-02,  1.1079e-01,  1.0260e-01,
        -4.5345e-03, -2.4260e-02, -8.5643e-02, -5.6693e-03, -9.5745e-02,
         7.2365e-02, -1.9980e-02,  1.0028e-01,  2.8137e-02,  6.3045e-03,
        -6.5949e-02,  1.7407e-02,  7.7145e-02,  1.1110e-01,  4.2881e-02,
        -1.4403e-01,  7.6998e-02,  2.7465e-02, -1.5967e-05, -8.8261e-02,
        -4.5001e-02,  3.7379e-03,  1.2376e-01, -3.4930e-02, -1.1700e-01,
        -7.0558e-02,  1.2190e-01,  6.9561e-02,  1.5748e-01, -2.4629e-02,
        -2.2651e-02,  1.0664e-01, -3.4575e-02, -5.0006e-02,  5.1855e-02,
         5.2601e-03,  1.8254e-01,  2.1461e-02, -3.7376e-02,  5.5724e-02,
         2.1181e-02,  1.4370e-02,  1.0543e-01,  2.6839e-02, -1.4981e-01,
         6.5579e-02, -7.6367e-02,  9.6859e-02, -1.2741e-01, -4.6638e-02,
        -1.1672e-02, -1.4133e-02,  1.0198e-01,  1.8374e-01, -5.9085e-03,
         9.8575e-02,  1.3069e-01,  3.8572e-02, -1.9284e-02, -7.2438e-02,
        -6.3737e-02, -4.8801e-02, -5.9923e-02, -3.3953e-03, -2.8542e-03,
         8.6536e-02,  1.3463e-01,  5.1801e-02, -8.8925e-03, -2.6540e-02,
         6.7430e-02,  1.1603e-01, -9.5293e-02,  1.0785e-01,  5.1305e-02,
        -1.4106e-01,  4.1780e-02, -5.6106e-02,  1.7975e-02, -7.5812e-02,
         4.4128e-02,  3.2993e-02,  8.2276e-02, -1.2963e-01, -4.6821e-02,
         5.1450e-02,  1.8220e-02, -8.5378e-02,  1.1224e-02,  1.8091e-02,
        -5.0665e-02,  9.0137e-02, -2.3424e-02,  3.7449e-02, -9.0868e-02,
        -5.1154e-02,  2.4289e-02,  1.0933e-01, -3.0303e-02, -7.6739e-02,
        -1.1506e-01,  4.6732e-02,  3.3456e-02, -4.5891e-02, -3.9702e-02,
         7.5385e-02,  6.9222e-02,  2.3683e-02, -3.0218e-02, -1.1337e-01,
        -1.0652e-01,  1.4910e-01,  4.4576e-02,  4.7361e-02,  3.8701e-02,
        -5.0572e-03, -1.5578e-01, -6.7457e-03, -3.2505e-02,  3.6560e-02,
         4.9492e-02, -2.2614e-02, -1.5328e-01,  1.1140e-01, -4.4644e-02,
         4.8380e-02,  1.2816e-01,  6.3836e-02,  1.3324e-01, -1.2345e-02,
        -2.1322e-02,  2.8828e-02, -3.9442e-02,  5.7570e-02, -1.0230e-01,
         1.0004e-02,  7.2100e-02,  3.9893e-02,  4.8353e-02, -1.8101e-02,
         9.2931e-03,  5.0046e-02, -7.8204e-02,  7.7226e-02,  5.9867e-02,
        -3.4463e-02,  4.0624e-02, -6.4362e-02,  7.8211e-02,  4.8034e-02,
         6.1703e-02, -1.0061e-01,  9.0966e-02,  1.9471e-02, -4.7507e-02,
         1.2117e-01, -4.4351e-02,  1.2515e-01,  8.8575e-02,  7.9516e-02,
         1.0305e-01, -1.9388e-01,  5.4264e-02,  3.4580e-02, -6.0438e-03,
         4.3602e-02, -1.9317e-02, -2.3738e-02,  3.9146e-02,  6.5333e-02,
        -8.7005e-02,  5.7420e-03,  1.2520e-02,  4.4381e-02, -1.9543e-02,
        -3.5200e-03, -4.2298e-03,  7.5359e-02, -2.3430e-02,  4.0863e-02,
        -3.1809e-02, -1.3691e-02,  6.0976e-03,  8.8307e-02,  7.1020e-02,
        -1.8984e-02, -4.6466e-02,  6.8843e-02,  8.8309e-02,  8.4172e-02,
         9.5667e-02,  8.3862e-02,  3.6680e-02,  1.0098e-01,  4.5047e-02,
         1.0024e-01, -1.5683e-01, -7.4672e-03,  7.5203e-02, -1.6064e-01,
        -2.8448e-02,  8.0379e-02, -5.3916e-03, -4.9619e-02,  5.7984e-02,
         1.5436e-01,  2.1874e-01,  6.1586e-02,  1.3023e-01,  4.0898e-02,
         8.0818e-03,  1.1342e-01,  1.0167e-01,  1.0800e-01,  5.5224e-03,
        -3.5651e-02, -9.9345e-03,  3.3143e-02, -5.5289e-02,  3.4321e-02,
         1.0234e-01], device='cuda:0', requires_grad=True)
using device 0 NVIDIA GeForce RTX 3090
DEVICE: cuda
client_num: 2
Traceback (most recent call last):
  File "/home/lxt/project/MVFIGN/v_main_mpc.py", line 73, in <module>
    user_features,user_labels,client_adj,label_num,train_features,train_labels,test_features,test_labels,at_f,at_l,mpc_beaver=deal_dataset(client_num,args.rate,args.r)
ValueError: not enough values to unpack (expected 11, got 10)
using device 0 NVIDIA GeForce RTX 3090
DEVICE: cuda
client_num: 2
Traceback (most recent call last):
  File "/home/lxt/project/MVFIGN/v_main_mpc.py", line 73, in <module>
    user_features,user_labels,client_adj,label_num,train_features,train_labels,test_features,test_labels,at_f,at_l,mpc_beaver=deal_dataset(client_num,args.rate,args.r)
ValueError: not enough values to unpack (expected 11, got 10)
using device 0 NVIDIA GeForce RTX 3090
DEVICE: cuda
client_num: 2
Initializing BFV encryption scheme...
Encryption initialization completed.

============================================================
Phase 1: Secure Local Training with MPC and Homomorphic Encryption
============================================================

--- Training Client 0 ---
Traceback (most recent call last):
  File "/home/lxt/project/MVFIGN/v_main_mpc.py", line 185, in <module>
    triplet_shares = mpc_beaver.generate_triplet(feature_shape)
  File "/home/lxt/project/MVFIGN/crypto_utils.py", line 44, in generate_triplet
    c = a @ b if len(shape) == 2 else a * b
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1895x1433 and 1895x1433)
using device 0 NVIDIA GeForce RTX 3090
DEVICE: cuda
client_num: 2
Initializing BFV encryption scheme...
Encryption initialization completed.

============================================================
Phase 1: Secure Local Training with MPC and Homomorphic Encryption
============================================================

--- Training Client 0 ---
Generated Beaver triplets for client 0: x(1895, 1433), w(1433, 1433)
Traceback (most recent call last):
  File "/home/lxt/project/MVFIGN/v_main_mpc.py", line 209, in <module>
    z_share = mpc_beaver.secure_matmul(
  File "/home/lxt/project/MVFIGN/crypto_utils.py", line 92, in secure_matmul
    alpha_share = x_share - a_share
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
using device 0 NVIDIA GeForce RTX 3090
DEVICE: cuda
client_num: 2
Initializing BFV encryption scheme...
Encryption initialization completed.

============================================================
Phase 1: Secure Local Training with MPC and Homomorphic Encryption
============================================================

--- Training Client 0 ---
Generated Beaver triplets for client 0: x(1895, 1433), w(1433, 1433)
tensor(30773946., device='cuda:0') tensor(34354900., device='cuda:0')
Traceback (most recent call last):
  File "/home/lxt/project/MVFIGN/v_main_mpc.py", line 222, in <module>
    loss.backward(retain_graph=True)
  File "/home/lxt/pytorch_env/lib/python3.9/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/lxt/pytorch_env/lib/python3.9/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
using device 0 NVIDIA GeForce RTX 3090
DEVICE: cuda
client_num: 2
Initializing BFV encryption scheme...
Encryption initialization completed.

============================================================
Phase 1: Local Training with Encrypted Parameter Transmission
============================================================

--- Training Client 0 ---
tensor(0.4348, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0212, device='cuda:0', grad_fn=<MeanBackward0>)
  Round 0/50, Loss: 0.4560
tensor(0.1886, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0182, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.2137, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0128, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.2300, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1746, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1093, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0065, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0898, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1094, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0074, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1241, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1097, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0814, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
  Round 10/50, Loss: 0.0900
tensor(0.0635, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0630, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0685, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0679, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0612, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0543, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0505, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0479, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0454, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0439, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
  Round 20/50, Loss: 0.0526
tensor(0.0443, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0440, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0409, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0365, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0342, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0350, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0360, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0347, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0322, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0308, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>)
  Round 30/50, Loss: 0.0391
tensor(0.0308, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0308, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0299, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0290, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0285, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0281, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0272, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0265, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0264, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0264, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)
  Round 40/50, Loss: 0.0349
tensor(0.0259, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0251, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0248, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0248, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0246, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0241, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0238, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0236, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0234, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)

[Client 0] Encrypting model parameters with BFV...
WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.
The following operations are disabled in this setup: matmul, matmul_plain, conv2d_im2col, replicate_first_slot.
If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.
  Encrypted parameter 'fc1.weight' with shape torch.Size([1433, 1433])
  Encrypted parameter 'fc1.bias' with shape torch.Size([1433])
[Client 0] Encryption completed. Total: 2 parameters.

[Server] Aggregating encrypted parameters from client 0...
[Clients] Receiving and decrypting aggregated parameters...
[Client 0] Parameters distributed securely to all clients.


--- Training Client 1 ---
tensor(0.4302, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0215, device='cuda:0', grad_fn=<MeanBackward0>)
  Round 0/50, Loss: 0.4516
tensor(0.1880, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.2144, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0128, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.2283, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1721, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1078, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0065, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0905, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1104, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0074, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1235, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.1080, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0801, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
  Round 10/50, Loss: 0.0887
tensor(0.0637, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0640, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0687, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0671, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0600, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0538, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0507, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0485, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0454, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0436, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)
  Round 20/50, Loss: 0.0523
tensor(0.0437, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0436, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0408, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0366, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0344, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0350, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0358, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0344, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0321, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0309, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
  Round 30/50, Loss: 0.0392
tensor(0.0310, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0307, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0298, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0289, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0285, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0281, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0272, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0264, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0264, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0264, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
  Round 40/50, Loss: 0.0350
tensor(0.0259, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0251, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0248, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0248, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0246, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0241, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0238, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0237, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)
tensor(0.0235, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>) tensor(0.0090, device='cuda:0', grad_fn=<MeanBackward0>)

[Client 1] Encrypting model parameters with BFV...
WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.
The following operations are disabled in this setup: matmul, matmul_plain, conv2d_im2col, replicate_first_slot.
If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.
  Encrypted parameter 'fc1.weight' with shape torch.Size([1433, 1433])
  Encrypted parameter 'fc1.bias' with shape torch.Size([1433])
[Client 1] Encryption completed. Total: 2 parameters.

[Server] Aggregating encrypted parameters from client 1...
[Clients] Receiving and decrypting aggregated parameters...
[Client 1] Parameters distributed securely to all clients.

============================================================
Phase 1 Completed: All clients trained with encrypted parameter sharing
============================================================

tran.0.fc1.weight
tran.0.fc1.bias
tran.1.fc1.weight
tran.1.fc1.bias
classifier.fc1.weight
classifier.fc1.bias
classifier.fc2.weight
classifier.fc2.bias
classifier.layernorm.weight
classifier.layernorm.bias
tran.0.fc1.weight
tran.0.fc1.bias
tran.1.fc1.weight
tran.1.fc1.bias
classifier.fc1.weight
classifier.fc1.bias
classifier.fc2.weight
classifier.fc2.bias
classifier.layernorm.weight
classifier.layernorm.bias
  0%|          | 0/300 [00:00<?, ?it/s]tensor(0.2866, dtype=torch.float64) 0.12767903969670671
tensor(0.3210, dtype=torch.float64) 0.27208920028810407
tensor(0.3419, dtype=torch.float64) 0.2285338501278135
tensor(0.3223, dtype=torch.float64) 0.24685069116538447
tensor(0.5068, dtype=torch.float64) 0.480728463078133
tensor(0.3272, dtype=torch.float64) 0.3242837029844946
tensor(0.3592, dtype=torch.float64) 0.3461740289798234
tensor(0.3592, dtype=torch.float64) 0.31419121310670245
tensor(0.3690, dtype=torch.float64) 0.3208769662035839
tensor(0.4231, dtype=torch.float64) 0.3087606122753236
tensor(0.4010, dtype=torch.float64) 0.31488958774344694
tensor(0.4034, dtype=torch.float64) 0.3286078477286258
tensor(0.4490, dtype=torch.float64) 0.38938007837037547
tensor(0.5474, dtype=torch.float64) 0.5110077627665927
tensor(0.5670, dtype=torch.float64) 0.5561503762471774
tensor(0.5966, dtype=torch.float64) 0.5859286226519762
tensor(0.6027, dtype=torch.float64) 0.5808647096366331
tensor(0.5843, dtype=torch.float64) 0.5601961954791239
tensor(0.5806, dtype=torch.float64) 0.5687400120090969
  6%|▋         | 19/300 [00:00<00:01, 188.99it/s]tensor(0.6138, dtype=torch.float64) 0.605298114355394
tensor(0.6630, dtype=torch.float64) 0.648393813041369
tensor(0.6593, dtype=torch.float64) 0.6415211886544082
tensor(0.6445, dtype=torch.float64) 0.633656547811628
tensor(0.6199, dtype=torch.float64) 0.6113716717247185
tensor(0.6199, dtype=torch.float64) 0.6144973344507154
tensor(0.6359, dtype=torch.float64) 0.6325738348433689
tensor(0.6581, dtype=torch.float64) 0.6551416208751932
tensor(0.6765, dtype=torch.float64) 0.673006645598002
tensor(0.6937, dtype=torch.float64) 0.6918245902178584
tensor(0.6974, dtype=torch.float64) 0.6966682302618452
tensor(0.7134, dtype=torch.float64) 0.713740503501571
tensor(0.7196, dtype=torch.float64) 0.7204042568079011
tensor(0.7245, dtype=torch.float64) 0.7235462059503949
tensor(0.7269, dtype=torch.float64) 0.7240103564629461
tensor(0.7196, dtype=torch.float64) 0.7146550158031695
tensor(0.7220, dtype=torch.float64) 0.7150231020757352
tensor(0.7183, dtype=torch.float64) 0.7120348034945698
tensor(0.7208, dtype=torch.float64) 0.7140830468959081
 13%|█▎        | 38/300 [00:00<00:01, 170.22it/s]tensor(0.7282, dtype=torch.float64) 0.7231404114459026
tensor(0.7319, dtype=torch.float64) 0.7275170389646682
tensor(0.7368, dtype=torch.float64) 0.7328880027981329
tensor(0.7392, dtype=torch.float64) 0.7358985736815394
tensor(0.7454, dtype=torch.float64) 0.7442788717739122
tensor(0.7528, dtype=torch.float64) 0.7532901967142444
tensor(0.7478, dtype=torch.float64) 0.7485396458973906
tensor(0.7442, dtype=torch.float64) 0.7449807291049331
tensor(0.7380, dtype=torch.float64) 0.7390637885273468
tensor(0.7306, dtype=torch.float64) 0.7316211612603903
tensor(0.7294, dtype=torch.float64) 0.7301318855794448
tensor(0.7269, dtype=torch.float64) 0.7274616880020678
tensor(0.7319, dtype=torch.float64) 0.7320219050735609
tensor(0.7405, dtype=torch.float64) 0.7403621350755285
tensor(0.7454, dtype=torch.float64) 0.7449756251233159
tensor(0.7478, dtype=torch.float64) 0.7471931132535164
tensor(0.7503, dtype=torch.float64) 0.7494942793658167
tensor(0.7540, dtype=torch.float64) 0.7529056955436991
 19%|█▊        | 56/300 [00:00<00:01, 161.90it/s]tensor(0.7577, dtype=torch.float64) 0.7562021403243385
tensor(0.7601, dtype=torch.float64) 0.7584450459824859
tensor(0.7565, dtype=torch.float64) 0.755000119279962
tensor(0.7626, dtype=torch.float64) 0.76151996820748
tensor(0.7577, dtype=torch.float64) 0.7567867355853604
tensor(0.7638, dtype=torch.float64) 0.7631213811118908
tensor(0.7589, dtype=torch.float64) 0.75813059689883
tensor(0.7589, dtype=torch.float64) 0.7583078664218896
tensor(0.7577, dtype=torch.float64) 0.7573537146520375
tensor(0.7577, dtype=torch.float64) 0.7574672444056806
tensor(0.7601, dtype=torch.float64) 0.7601315679485615
tensor(0.7651, dtype=torch.float64) 0.7651811305142697
tensor(0.7651, dtype=torch.float64) 0.7653795854518586
tensor(0.7663, dtype=torch.float64) 0.7666306152882083
tensor(0.7688, dtype=torch.float64) 0.7694426017242612
tensor(0.7688, dtype=torch.float64) 0.7692589478301795
tensor(0.7724, dtype=torch.float64) 0.7724648332965002
 24%|██▍       | 73/300 [00:00<00:01, 158.37it/s]tensor(0.7724, dtype=torch.float64) 0.771824184751154
tensor(0.7724, dtype=torch.float64) 0.7714208483437691
tensor(0.7712, dtype=torch.float64) 0.7701371477967245
tensor(0.7688, dtype=torch.float64) 0.7673712235223621
tensor(0.7700, dtype=torch.float64) 0.768789569737966
tensor(0.7663, dtype=torch.float64) 0.7652287803573784
tensor(0.7675, dtype=torch.float64) 0.7664199830056101
tensor(0.7675, dtype=torch.float64) 0.7667237974335535
tensor(0.7675, dtype=torch.float64) 0.7669554506825561
tensor(0.7712, dtype=torch.float64) 0.7716351532213248
tensor(0.7700, dtype=torch.float64) 0.7704104828961671
tensor(0.7700, dtype=torch.float64) 0.7704104828961671
tensor(0.7700, dtype=torch.float64) 0.7709105571750988
tensor(0.7663, dtype=torch.float64) 0.7670062972615309
tensor(0.7651, dtype=torch.float64) 0.7653432660597013
tensor(0.7675, dtype=torch.float64) 0.7679328568009954
 30%|██▉       | 89/300 [00:00<00:01, 156.31it/s]tensor(0.7675, dtype=torch.float64) 0.767389777528833
tensor(0.7638, dtype=torch.float64) 0.7633814136891696
tensor(0.7638, dtype=torch.float64) 0.7635300187396841
tensor(0.7651, dtype=torch.float64) 0.764644297251384
tensor(0.7638, dtype=torch.float64) 0.7633616492586685
tensor(0.7614, dtype=torch.float64) 0.7609315182197354
tensor(0.7638, dtype=torch.float64) 0.7636193769580961
tensor(0.7614, dtype=torch.float64) 0.761377524943008
tensor(0.7589, dtype=torch.float64) 0.7588700842336362
tensor(0.7577, dtype=torch.float64) 0.7574258481108619
tensor(0.7577, dtype=torch.float64) 0.7575146951682263
tensor(0.7540, dtype=torch.float64) 0.7535080745030714
tensor(0.7540, dtype=torch.float64) 0.7538098239650471
tensor(0.7540, dtype=torch.float64) 0.7537797070157553
tensor(0.7552, dtype=torch.float64) 0.7550950891071193
tensor(0.7614, dtype=torch.float64) 0.7609439291188895
 35%|███▌      | 105/300 [00:00<00:01, 155.18it/s]tensor(0.7614, dtype=torch.float64) 0.7611025620906895
tensor(0.7577, dtype=torch.float64) 0.7575962099410902
tensor(0.7601, dtype=torch.float64) 0.7599046670138834
tensor(0.7565, dtype=torch.float64) 0.7559072145925263
tensor(0.7577, dtype=torch.float64) 0.7572176183725555
tensor(0.7565, dtype=torch.float64) 0.7563094289761483
tensor(0.7515, dtype=torch.float64) 0.7512392303662154
tensor(0.7528, dtype=torch.float64) 0.7524670701468118
tensor(0.7552, dtype=torch.float64) 0.7548470281602482
tensor(0.7515, dtype=torch.float64) 0.7516661781841248
tensor(0.7552, dtype=torch.float64) 0.7553370050642678
tensor(0.7491, dtype=torch.float64) 0.7490470525317038
tensor(0.7466, dtype=torch.float64) 0.7462942407078218
tensor(0.7466, dtype=torch.float64) 0.7460547022205498
tensor(0.7454, dtype=torch.float64) 0.7449548560863463
tensor(0.7491, dtype=torch.float64) 0.7487910900951561
 40%|████      | 121/300 [00:00<00:01, 154.37it/s]tensor(0.7491, dtype=torch.float64) 0.7489404021285583
tensor(0.7515, dtype=torch.float64) 0.7515667480638675
tensor(0.7503, dtype=torch.float64) 0.750123938291127
tensor(0.7478, dtype=torch.float64) 0.7475598885611847
tensor(0.7429, dtype=torch.float64) 0.7426115899736143
tensor(0.7454, dtype=torch.float64) 0.7451223649366385
tensor(0.7491, dtype=torch.float64) 0.7480695902012204
tensor(0.7478, dtype=torch.float64) 0.74656429801119
tensor(0.7528, dtype=torch.float64) 0.7516330074588139
tensor(0.7540, dtype=torch.float64) 0.753958327886265
tensor(0.7491, dtype=torch.float64) 0.748832631779358
tensor(0.7478, dtype=torch.float64) 0.7475412195006517
tensor(0.7491, dtype=torch.float64) 0.7482692701614276
tensor(0.7515, dtype=torch.float64) 0.7506606863439207
tensor(0.7552, dtype=torch.float64) 0.7539095791954108
tensor(0.7515, dtype=torch.float64) 0.750331053848135
tensor(0.7565, dtype=torch.float64) 0.755680108378406
 46%|████▌     | 138/300 [00:00<00:01, 156.99it/s]tensor(0.7515, dtype=torch.float64) 0.751683258155498
tensor(0.7503, dtype=torch.float64) 0.7504693472992905
tensor(0.7540, dtype=torch.float64) 0.753550301085026
tensor(0.7540, dtype=torch.float64) 0.7525915103239986
tensor(0.7565, dtype=torch.float64) 0.7545644712308387
tensor(0.7528, dtype=torch.float64) 0.7517308619466772
tensor(0.7503, dtype=torch.float64) 0.7506535502966262
tensor(0.7491, dtype=torch.float64) 0.7490062378620168
tensor(0.7528, dtype=torch.float64) 0.7519343788007036
tensor(0.7478, dtype=torch.float64) 0.7481749147379819
tensor(0.7478, dtype=torch.float64) 0.7485361296860555
tensor(0.7528, dtype=torch.float64) 0.7520473763090992
tensor(0.7552, dtype=torch.float64) 0.7543321449426789
tensor(0.7454, dtype=torch.float64) 0.7459471240690969
tensor(0.7515, dtype=torch.float64) 0.7514369771588624
tensor(0.7515, dtype=torch.float64) 0.7502683020389171
tensor(0.7552, dtype=torch.float64) 0.7544329649990869
tensor(0.7478, dtype=torch.float64) 0.7476075707184607
tensor(0.7478, dtype=torch.float64) 0.74715275634456
 52%|█████▏    | 157/300 [00:00<00:00, 165.34it/s]tensor(0.7515, dtype=torch.float64) 0.7508992891351813
tensor(0.7565, dtype=torch.float64) 0.7567406833846879
tensor(0.7503, dtype=torch.float64) 0.7509273881137619
tensor(0.7454, dtype=torch.float64) 0.743050143542948
tensor(0.7491, dtype=torch.float64) 0.7480484598690027
tensor(0.7429, dtype=torch.float64) 0.7427667481274107
tensor(0.7282, dtype=torch.float64) 0.7297719608443846
tensor(0.7442, dtype=torch.float64) 0.7442529239473064
tensor(0.7528, dtype=torch.float64) 0.7508052806225883
tensor(0.7491, dtype=torch.float64) 0.7489271938960662
tensor(0.7454, dtype=torch.float64) 0.7449936244550563
tensor(0.7442, dtype=torch.float64) 0.7426151637145966
tensor(0.7503, dtype=torch.float64) 0.7505774312697042
tensor(0.7442, dtype=torch.float64) 0.7454954821020975
tensor(0.7454, dtype=torch.float64) 0.7455760541662961
tensor(0.7417, dtype=torch.float64) 0.7410482116513364
tensor(0.7442, dtype=torch.float64) 0.7451993666472329
tensor(0.7466, dtype=torch.float64) 0.7455810084403324
tensor(0.7565, dtype=torch.float64) 0.7550785286203932
 59%|█████▊    | 176/300 [00:01<00:00, 171.56it/s]tensor(0.7331, dtype=torch.float64) 0.7354209401082435
tensor(0.7429, dtype=torch.float64) 0.7416127030875466
tensor(0.7565, dtype=torch.float64) 0.7548094031298194
tensor(0.7380, dtype=torch.float64) 0.7392994816444565
tensor(0.7491, dtype=torch.float64) 0.7486656590826121
tensor(0.7478, dtype=torch.float64) 0.7474564278185387
tensor(0.7528, dtype=torch.float64) 0.7525976356896278
tensor(0.7491, dtype=torch.float64) 0.7494600951627852
tensor(0.7503, dtype=torch.float64) 0.7497107154333078
tensor(0.7368, dtype=torch.float64) 0.7354212883342355
tensor(0.7331, dtype=torch.float64) 0.7342392895237383
tensor(0.7540, dtype=torch.float64) 0.7538383194651292
tensor(0.7515, dtype=torch.float64) 0.7500795406778553
tensor(0.7380, dtype=torch.float64) 0.7392507001875257
tensor(0.7368, dtype=torch.float64) 0.7370411820765322
tensor(0.7429, dtype=torch.float64) 0.7413511295448592
tensor(0.7405, dtype=torch.float64) 0.7386323793119396
tensor(0.7368, dtype=torch.float64) 0.7389932313377439
tensor(0.7269, dtype=torch.float64) 0.7307497320798458
 65%|██████▌   | 195/300 [00:01<00:00, 176.03it/s]tensor(0.7466, dtype=torch.float64) 0.7457274368861481
tensor(0.7528, dtype=torch.float64) 0.7513752418203183
tensor(0.7392, dtype=torch.float64) 0.7404237697525935
tensor(0.7392, dtype=torch.float64) 0.7409860198953591
tensor(0.7442, dtype=torch.float64) 0.743649537003856
tensor(0.7491, dtype=torch.float64) 0.7480830786118808
tensor(0.7405, dtype=torch.float64) 0.7413268327253419
tensor(0.7319, dtype=torch.float64) 0.7323280575956107
tensor(0.7319, dtype=torch.float64) 0.7307158536267108
tensor(0.7478, dtype=torch.float64) 0.7468714740573158
tensor(0.7368, dtype=torch.float64) 0.7369121406694404
tensor(0.7331, dtype=torch.float64) 0.7339775549214154
tensor(0.7392, dtype=torch.float64) 0.7385665371578225
tensor(0.7368, dtype=torch.float64) 0.7365341475550962
tensor(0.7405, dtype=torch.float64) 0.742264462334095
tensor(0.7392, dtype=torch.float64) 0.7405890418713967
tensor(0.7319, dtype=torch.float64) 0.7304539364610316
tensor(0.7405, dtype=torch.float64) 0.7377705273200839
tensor(0.7405, dtype=torch.float64) 0.7405987512043561
 71%|███████▏  | 214/300 [00:01<00:00, 179.31it/s]tensor(0.7269, dtype=torch.float64) 0.7303478564490773
tensor(0.7380, dtype=torch.float64) 0.7389538509199638
tensor(0.7454, dtype=torch.float64) 0.743888298075598
tensor(0.7454, dtype=torch.float64) 0.7453271965839481
tensor(0.7294, dtype=torch.float64) 0.7306324538102272
tensor(0.7331, dtype=torch.float64) 0.7329026415000786
tensor(0.7429, dtype=torch.float64) 0.7403099760072488
tensor(0.7466, dtype=torch.float64) 0.7464194912680783
tensor(0.7417, dtype=torch.float64) 0.7437976627946712
tensor(0.7343, dtype=torch.float64) 0.7361241824533976
tensor(0.7405, dtype=torch.float64) 0.740453042128679
tensor(0.7503, dtype=torch.float64) 0.7495628157971344
tensor(0.7491, dtype=torch.float64) 0.7484700328156791
tensor(0.7454, dtype=torch.float64) 0.7460205541819847
tensor(0.7319, dtype=torch.float64) 0.7319903530410286
tensor(0.7392, dtype=torch.float64) 0.7382211174743601
tensor(0.7478, dtype=torch.float64) 0.7460967209048965
tensor(0.7540, dtype=torch.float64) 0.7535036260824965
tensor(0.7405, dtype=torch.float64) 0.7420373641895264
 78%|███████▊  | 233/300 [00:01<00:00, 181.69it/s]tensor(0.7343, dtype=torch.float64) 0.7345763051065848
tensor(0.7380, dtype=torch.float64) 0.7369179923645479
tensor(0.7466, dtype=torch.float64) 0.7457347658689046
tensor(0.7429, dtype=torch.float64) 0.7437273162757769
tensor(0.7392, dtype=torch.float64) 0.7391851511084009
tensor(0.7454, dtype=torch.float64) 0.743663000437249
tensor(0.7368, dtype=torch.float64) 0.7339392679985388
tensor(0.7319, dtype=torch.float64) 0.731745975578724
tensor(0.7245, dtype=torch.float64) 0.727038574897416
tensor(0.7355, dtype=torch.float64) 0.7351329575359592
tensor(0.7429, dtype=torch.float64) 0.7418368575996909
tensor(0.7319, dtype=torch.float64) 0.7323541615581465
tensor(0.7405, dtype=torch.float64) 0.7402832995630846
tensor(0.7392, dtype=torch.float64) 0.7385927330915417
tensor(0.7294, dtype=torch.float64) 0.7284330009588672
tensor(0.7368, dtype=torch.float64) 0.7369214082012377
tensor(0.7294, dtype=torch.float64) 0.7324165477674457
tensor(0.7442, dtype=torch.float64) 0.7419430958411141
tensor(0.7355, dtype=torch.float64) 0.7333388403299886
 84%|████████▍ | 252/300 [00:01<00:00, 182.49it/s]tensor(0.7331, dtype=torch.float64) 0.7347140018003641
tensor(0.7417, dtype=torch.float64) 0.7430953221037573
tensor(0.7466, dtype=torch.float64) 0.7446423318221479
tensor(0.7355, dtype=torch.float64) 0.7340962024730749
tensor(0.7380, dtype=torch.float64) 0.7393557430265197
tensor(0.7405, dtype=torch.float64) 0.7414670514616479
tensor(0.7368, dtype=torch.float64) 0.7360889002557717
tensor(0.7368, dtype=torch.float64) 0.7360360027405815
tensor(0.7405, dtype=torch.float64) 0.7408703161544362
tensor(0.7380, dtype=torch.float64) 0.7388053172563865
tensor(0.7380, dtype=torch.float64) 0.7384883883114931
tensor(0.7392, dtype=torch.float64) 0.7389704305891676
tensor(0.7417, dtype=torch.float64) 0.7417526092740486
tensor(0.7429, dtype=torch.float64) 0.7437827824423113
tensor(0.7429, dtype=torch.float64) 0.7438409865654385
tensor(0.7331, dtype=torch.float64) 0.732249684556965
tensor(0.7380, dtype=torch.float64) 0.7376944312555747
tensor(0.7343, dtype=torch.float64) 0.7348420870746648
tensor(0.7257, dtype=torch.float64) 0.7278195751004976
 90%|█████████ | 271/300 [00:01<00:00, 183.36it/s]tensor(0.7306, dtype=torch.float64) 0.7319155944504928
tensor(0.7454, dtype=torch.float64) 0.7441448291523385
tensor(0.7405, dtype=torch.float64) 0.7390951351496879
tensor(0.7442, dtype=torch.float64) 0.7437495644073456
tensor(0.7319, dtype=torch.float64) 0.7330097120708456
tensor(0.7429, dtype=torch.float64) 0.7441966846705752
tensor(0.7491, dtype=torch.float64) 0.7479815008992737
tensor(0.7491, dtype=torch.float64) 0.7464804041594703
tensor(0.7306, dtype=torch.float64) 0.7319331685892866
tensor(0.7282, dtype=torch.float64) 0.7311055726304709
tensor(0.7380, dtype=torch.float64) 0.7406693531453304
tensor(0.7478, dtype=torch.float64) 0.7455581233140227
tensor(0.7208, dtype=torch.float64) 0.7217152289705006
tensor(0.7036, dtype=torch.float64) 0.7034356247581786
tensor(0.6851, dtype=torch.float64) 0.7022646971892423
tensor(0.7023, dtype=torch.float64) 0.6820540875367058
tensor(0.6654, dtype=torch.float64) 0.6778558557434529
tensor(0.7405, dtype=torch.float64) 0.7416037122681257
tensor(0.7245, dtype=torch.float64) 0.7184713441217837
 97%|█████████▋| 290/300 [00:01<00:00, 184.03it/s]tensor(0.7319, dtype=torch.float64) 0.7290732293541736
tensor(0.7355, dtype=torch.float64) 0.7370550525359619
tensor(0.7220, dtype=torch.float64) 0.7308713284779625
tensor(0.7257, dtype=torch.float64) 0.7275955537444453
tensor(0.7429, dtype=torch.float64) 0.7422782471019096
tensor(0.7380, dtype=torch.float64) 0.7378383597047642
tensor(0.7491, dtype=torch.float64) 0.7495503546243899
tensor(0.7528, dtype=torch.float64) 0.7532604838561298
tensor(0.7429, dtype=torch.float64) 0.7446716187948429
tensor(0.7269, dtype=torch.float64) 0.7283911412466267
100%|██████████| 300/300 [00:01<00:00, 172.34it/s]
tran.0.fc1.weight Parameter containing:
tensor([[ 0.2130, -0.0010,  0.0000,  ..., -0.0010, -0.0010, -0.0020],
        [-0.0020,  0.3190, -0.0050,  ..., -0.0040, -0.0070, -0.0030],
        [ 0.0020, -0.0140,  0.3340,  ..., -0.0050, -0.0070,  0.0040],
        ...,
        [ 0.0000, -0.0020,  0.0000,  ...,  0.1800,  0.0040, -0.0030],
        [ 0.0070,  0.0020,  0.0080,  ...,  0.0120,  0.3620, -0.0020],
        [-0.0010,  0.0000, -0.0020,  ..., -0.0050, -0.0030,  0.1890]],
       device='cuda:0')
tran.0.fc1.bias Parameter containing:
tensor([ 0.0010,  0.0030,  0.0040,  ...,  0.0010, -0.0030,  0.0020],
       device='cuda:0')
tran.1.fc1.weight Parameter containing:
tensor([[ 0.2160,  0.0010,  0.0070,  ...,  0.0030,  0.0050,  0.0000],
        [-0.0010,  0.3180, -0.0010,  ..., -0.0020, -0.0050, -0.0020],
        [ 0.0030, -0.0090,  0.3510,  ..., -0.0010, -0.0040,  0.0090],
        ...,
        [ 0.0000,  0.0020,  0.0030,  ...,  0.1840,  0.0090,  0.0000],
        [ 0.0020, -0.0030,  0.0050,  ...,  0.0110,  0.3360, -0.0040],
        [-0.0030, -0.0050, -0.0070,  ..., -0.0070, -0.0070,  0.1880]],
       device='cuda:0')
tran.1.fc1.bias Parameter containing:
tensor([-0.0030,  0.0020,  0.0000,  ..., -0.0020, -0.0020,  0.0040],
       device='cuda:0')
classifier.fc1.weight Parameter containing:
tensor([[-0.0137, -0.0739,  0.0979,  ...,  0.0161,  0.0402, -0.0182],
        [ 0.0140, -0.0618, -0.0150,  ..., -0.0041, -0.0543,  0.0013],
        [ 0.0087,  0.0150,  0.0174,  ...,  0.0158, -0.0024, -0.0022],
        ...,
        [-0.0167,  0.0106, -0.0329,  ..., -0.0062,  0.0129, -0.0042],
        [-0.0136, -0.0339,  0.0678,  ..., -0.0229, -0.0259,  0.0117],
        [-0.0069,  0.0225,  0.0191,  ..., -0.0153, -0.0571, -0.0060]],
       device='cuda:0', requires_grad=True)
classifier.fc1.bias Parameter containing:
tensor([ 9.3134e-03,  1.3758e-02, -1.5013e-02,  1.1471e-02,  5.8043e-03,
         7.4803e-03, -1.0226e-02,  1.0531e-02, -3.9698e-03, -1.0396e-02,
        -3.0445e-03,  1.4743e-02, -5.3058e-03,  2.0023e-04,  1.2636e-02,
         1.8977e-02,  1.6885e-02,  1.4547e-02,  3.1853e-03,  3.1149e-03,
        -5.6222e-03,  4.0378e-03, -3.1636e-03, -1.4122e-02,  1.5921e-02,
        -5.7536e-03, -1.1927e-03, -8.2276e-03,  4.6888e-03, -1.2176e-02,
        -4.8136e-03, -6.0128e-03, -2.3214e-03, -1.9055e-02,  1.1686e-02,
         1.0874e-02,  6.5073e-03,  8.5610e-03,  7.1740e-03,  1.0253e-02,
        -1.1613e-02, -1.0213e-02, -8.1540e-03,  1.1068e-03, -9.1768e-03,
        -4.8547e-03,  2.6232e-03, -7.9624e-03, -9.7067e-03, -1.3330e-02,
         7.5033e-03, -6.7572e-03, -7.4091e-03, -2.3595e-02, -1.5793e-02,
         2.2511e-02,  6.9419e-03, -8.0233e-03, -1.4498e-02,  1.5092e-02,
        -2.2674e-03,  7.1062e-03, -7.1470e-03,  6.2835e-03,  6.0939e-03,
        -6.6021e-03,  1.3150e-02,  4.0024e-03, -2.0665e-04,  1.1469e-02,
         1.4213e-02,  1.2666e-02,  1.1370e-02, -7.2301e-03,  1.6784e-03,
        -1.3691e-02,  1.2456e-02, -8.4295e-03, -7.9061e-03, -8.4317e-03,
        -5.4289e-03,  8.3265e-03,  3.0388e-03, -5.9975e-04, -4.9692e-03,
         6.9348e-03,  5.2742e-03, -8.3514e-03, -1.5704e-02, -1.0125e-02,
        -6.0840e-03,  6.9371e-03,  8.4935e-03,  1.4959e-02, -1.5204e-02,
         9.5883e-03,  9.8419e-03,  6.9644e-03, -1.4255e-02,  1.3406e-02,
        -7.0392e-03,  1.4615e-02,  1.3246e-02, -7.8034e-03, -9.5601e-03,
         1.6287e-02, -1.1926e-02,  8.7275e-03, -1.6529e-03, -1.1229e-02,
        -1.1541e-02, -1.6460e-02, -1.3717e-02,  1.4701e-02,  3.6755e-03,
         1.0048e-02,  7.3325e-03, -9.3885e-03,  4.3153e-03, -1.0350e-02,
        -9.1062e-03, -3.5887e-02,  3.2467e-04,  1.0399e-02,  5.7697e-03,
        -1.1121e-02,  4.1104e-03, -1.3418e-02,  5.4804e-03,  8.6117e-03,
         4.7467e-03,  1.6614e-02, -1.5337e-02,  1.1531e-02, -1.2528e-02,
         7.6522e-03,  1.2611e-02,  8.4253e-03, -1.4155e-02, -1.7188e-03,
        -1.6020e-02,  9.6118e-03, -1.0142e-04, -7.4205e-03,  1.3733e-02,
         7.7756e-04,  1.3232e-02, -8.2392e-03, -1.3269e-02,  1.5464e-02,
         2.1043e-02, -6.6122e-03,  8.0833e-03, -7.9664e-03, -6.1186e-03,
         1.7797e-02, -1.1701e-02,  5.6969e-03, -1.3614e-02,  7.3719e-03,
         1.1745e-02, -3.9514e-03,  2.2220e-03,  6.3264e-03, -7.0877e-03,
         8.5609e-03,  1.2192e-02,  5.5716e-03,  6.7106e-03, -4.9672e-03,
        -2.2342e-03, -1.2153e-02,  1.2790e-02,  1.5670e-02, -1.0451e-02,
        -3.5758e-03, -7.0982e-03, -5.6653e-03,  4.4915e-03, -7.7186e-04,
         8.9164e-03,  1.1338e-02, -1.3304e-02,  5.8376e-03,  1.2529e-02,
         7.3569e-03,  4.8856e-03,  1.2572e-02, -1.2511e-02,  1.0100e-02,
         1.3066e-02,  8.4046e-03, -1.0108e-02, -1.5698e-03, -5.1900e-03,
        -1.6612e-02,  1.5273e-02,  1.4059e-02, -4.3686e-03, -7.5277e-03,
         1.1728e-02, -8.8534e-03, -4.1628e-03,  1.1677e-02, -1.1332e-02,
         3.2657e-03,  1.2724e-02, -1.4941e-02, -1.3815e-02, -1.2515e-02,
         7.2294e-03, -9.4300e-03, -1.5054e-03, -1.7512e-03, -1.5019e-02,
        -8.1560e-03, -1.5162e-02, -8.9857e-03, -1.2252e-02, -4.7918e-03,
        -2.6085e-03,  9.1212e-03, -1.2845e-02, -4.3720e-03,  4.5281e-03,
         6.6405e-03,  5.1591e-03, -1.3577e-03, -1.1982e-02,  7.7215e-03,
         3.3106e-03, -1.0983e-02,  1.8294e-02,  8.2765e-03, -2.1365e-03,
         1.1936e-02, -7.8475e-03,  6.2857e-03,  9.2774e-03, -9.0469e-03,
        -1.2305e-02,  6.6835e-03,  1.1389e-02,  1.2244e-02,  5.5160e-03,
        -2.5837e-03,  1.7869e-02,  3.5145e-03, -1.6337e-05,  3.9006e-03,
        -3.3565e-03,  2.4253e-03, -1.1057e-02, -3.8088e-03, -1.8518e-03,
        -5.8207e-03], device='cuda:0', requires_grad=True)
classifier.fc2.weight Parameter containing:
tensor([[ 0.1459,  0.1351, -0.0377,  ..., -0.0201,  0.0184, -0.0386],
        [-0.1891, -0.1476,  0.0399,  ..., -0.0020, -0.1615,  0.0943],
        [-0.0871,  0.1074, -0.0366,  ...,  0.1528, -0.1239,  0.0271],
        ...,
        [ 0.1477,  0.0699, -0.0935,  ..., -0.0603,  0.0777,  0.0941],
        [ 0.0937, -0.1500, -0.0718,  ...,  0.0487, -0.1053, -0.0515],
        [-0.0048,  0.0859,  0.0041,  ..., -0.0731, -0.0518, -0.0351]],
       device='cuda:0', requires_grad=True)
classifier.fc2.bias Parameter containing:
tensor([ 0.0094, -0.0126,  0.0102,  0.0191,  0.0067,  0.0327,  0.0028],
       device='cuda:0', requires_grad=True)
classifier.layernorm.weight Parameter containing:
tensor([0.8529, 0.8486, 0.7745, 0.7547, 0.6040, 0.9285, 0.8237, 0.8493, 0.8673,
        0.8888, 0.9442, 0.8330, 0.9163, 0.8431, 0.7857, 0.7616, 0.7758, 0.7171,
        0.9826, 0.9181, 0.9323, 0.9436, 0.7596, 0.6585, 0.8254, 0.8221, 0.7451,
        0.9264, 0.7519, 0.8506, 0.8548, 0.8407, 0.9177, 0.7118, 0.7530, 0.7372,
        0.9233, 0.7645, 0.8594, 0.8459, 0.8218, 0.8519, 0.8567, 0.8304, 0.8146,
        0.8956, 0.9181, 0.7458, 0.9500, 0.8083, 0.8125, 0.7537, 0.8213, 0.7372,
        0.7260, 0.7169, 0.8465, 0.9125, 0.7589, 0.7761, 0.6922, 0.7961, 0.7478,
        0.8847, 0.6160, 0.7066, 0.6368, 0.9387, 0.8145, 0.8884, 0.8821, 0.7703,
        0.7624, 0.7971, 0.8848, 0.8115, 0.8483, 0.8465, 0.9201, 0.6201, 0.9461,
        0.8299, 0.9892, 0.8789, 0.8517, 0.9270, 0.8046, 0.8349, 0.7014, 0.7257,
        0.8579, 0.9339, 0.7889, 0.7844, 0.7772, 0.8829, 0.6993, 0.8066, 0.7260,
        0.7552, 0.7832, 0.8072, 0.7351, 0.7008, 0.8265, 0.9137, 0.9265, 0.9215,
        0.8181, 0.7931, 0.6362, 0.7009, 0.7002, 0.9224, 0.8393, 0.8065, 0.8548,
        0.6979, 0.7958, 0.8057, 0.6580, 0.7364, 0.8904, 0.8232, 0.9043, 0.8211,
        0.7426, 0.7444, 0.7966, 0.7713, 0.6607, 0.6543, 0.7604, 0.6487, 0.8101,
        0.7742, 0.7829, 0.9592, 0.7201, 0.9881, 0.7904, 0.8191, 0.6575, 0.8485,
        0.6308, 0.7659, 0.7312, 0.9272, 0.7814, 0.7546, 0.8259, 0.8219, 0.7915,
        0.8580, 0.9257, 0.6189, 0.8402, 0.9294, 0.7837, 0.8223, 0.7855, 0.8726,
        0.8520, 0.7739, 0.8427, 0.8395, 0.7461, 0.8603, 0.7774, 0.7919, 0.8149,
        0.7017, 0.9523, 0.9388, 0.7921, 0.8129, 0.7912, 0.8532, 0.8690, 0.8560,
        0.8298, 0.9176, 0.8241, 0.8351, 0.9611, 0.8601, 0.9618, 0.8371, 0.7071,
        0.9874, 0.9240, 0.8215, 0.7097, 1.0433, 0.7746, 0.8306, 0.8232, 0.8295,
        0.7049, 0.8352, 0.7447, 0.8853, 0.8928, 0.8399, 0.7052, 0.8803, 0.8274,
        0.6555, 0.7682, 0.7749, 0.7487, 0.7882, 0.8253, 0.6062, 0.7943, 0.8096,
        0.7740, 0.8324, 0.8586, 0.9558, 0.9479, 0.9018, 0.8747, 0.8471, 0.9156,
        0.8311, 0.9862, 0.9236, 0.8504, 0.7401, 0.7674, 0.8820, 0.7298, 0.9364,
        0.7838, 0.8737, 0.7472, 0.9458, 0.8407, 0.8354, 0.6087, 0.9705, 0.8520,
        0.7952, 0.9584, 0.7673, 0.8078, 1.0496, 0.8953, 0.7945, 0.8859, 0.7447,
        0.8075, 0.7772, 0.7957, 0.8239], device='cuda:0', requires_grad=True)
classifier.layernorm.bias Parameter containing:
tensor([-0.1170, -0.1484,  0.1106, -0.1910, -0.1509, -0.0244,  0.1055, -0.1419,
         0.0433,  0.1028, -0.0361, -0.1144, -0.1120, -0.1231, -0.1204, -0.1292,
        -0.0919, -0.1497, -0.0511, -0.0105, -0.0167, -0.0961,  0.0949,  0.1270,
        -0.1261,  0.0492,  0.0957,  0.0241, -0.1192,  0.1060,  0.0731,  0.1188,
         0.0569,  0.1425, -0.1048, -0.1408, -0.0699, -0.1342, -0.1452, -0.1352,
         0.0873, -0.0691,  0.1114, -0.0500,  0.1284,  0.1018,  0.0385,  0.0878,
         0.0668,  0.0883, -0.1025,  0.1137,  0.0854,  0.1180,  0.1042, -0.1551,
        -0.0929,  0.0505,  0.1029, -0.1047,  0.1542, -0.0981,  0.1099, -0.0907,
        -0.0043,  0.1272, -0.1533, -0.0789,  0.1077, -0.0730, -0.1046, -0.1361,
        -0.1185,  0.0848, -0.0037,  0.1134, -0.1640,  0.1146,  0.0760,  0.1569,
         0.0700, -0.0681, -0.0470, -0.0968, -0.0386, -0.0600,  0.0926,  0.1024,
         0.1252,  0.1295,  0.0411, -0.0639, -0.1330, -0.1532,  0.0917, -0.0738,
        -0.1488, -0.1095,  0.1608, -0.0910, -0.1353, -0.1286,  0.1038,  0.1297,
         0.0944, -0.1321, -0.0229, -0.0661, -0.0625,  0.1026,  0.1191,  0.1174,
         0.1095, -0.0998, -0.0846, -0.1088, -0.0394,  0.1257, -0.0384,  0.1306,
         0.1146,  0.1121,  0.0374, -0.0841, -0.0831,  0.1213, -0.1521,  0.1248,
        -0.1140, -0.1701, -0.1948, -0.1575,  0.0847, -0.1439,  0.0876, -0.1554,
        -0.1487, -0.0358,  0.1353,  0.0231,  0.0842, -0.1124,  0.0771,  0.0959,
        -0.1454, -0.0543, -0.1629,  0.0944,  0.1237, -0.1133, -0.0951,  0.0573,
        -0.1268,  0.1260,  0.0539, -0.1258,  0.1135, -0.0631,  0.1037, -0.1149,
        -0.1068, -0.0487, -0.0580, -0.1407,  0.0955, -0.1108, -0.1369, -0.1014,
        -0.1459,  0.0573,  0.1068,  0.1468, -0.0866, -0.1190,  0.1090,  0.0693,
         0.1126,  0.0451, -0.0823,  0.0559, -0.0972, -0.0727,  0.1045,  0.0189,
        -0.1168, -0.0944, -0.0428, -0.1380,  0.1079, -0.0106, -0.0955, -0.1316,
         0.0670, -0.0329,  0.1178,  0.1049, -0.1435, -0.1083,  0.1337,  0.0266,
        -0.1198,  0.0801,  0.0675, -0.1466,  0.1493, -0.0910, -0.0917,  0.1497,
         0.1432,  0.0814, -0.1305,  0.1103,  0.0298,  0.1386,  0.0916,  0.1020,
         0.1247,  0.0880,  0.0929, -0.0389,  0.0314, -0.0956,  0.1101,  0.0604,
         0.0307, -0.0793, -0.0797,  0.0494,  0.1020, -0.1591, -0.1194,  0.0876,
        -0.1398, -0.1202,  0.1206, -0.1459,  0.1192, -0.0751, -0.1549,  0.0774,
         0.1274, -0.0719, -0.0600, -0.1094, -0.0091, -0.1442, -0.1073, -0.0655,
        -0.0424,  0.0932,  0.0760,  0.1677,  0.0902,  0.1212,  0.1007,  0.0975],
       device='cuda:0', requires_grad=True)
using device 0 NVIDIA GeForce RTX 3090
DEVICE: cuda
client_num: 2
