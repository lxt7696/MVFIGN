# v_model_mpc.py - å®Œæ•´ç‰ˆï¼ˆä½¿ç”¨åˆ†å— MPCï¼‰

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Dropout, Linear, LayerNorm
from torch.autograd import Variable
from mpc_utils import MPSPDZManager

class Mlp(nn.Module):
    def __init__(self, input_dim, hid_dim, c_dim, dropout):
        super(Mlp, self).__init__()
        self.fc1 = Linear(input_dim, hid_dim)
        self.fc2 = Linear(hid_dim, c_dim)
        self.act_fn = torch.nn.functional.gelu
        self._init_weights()
        self.dropout = Dropout(dropout)
        self.layernorm = LayerNorm(hid_dim, eps=1e-6)
    
    def _init_weights(self):
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.xavier_uniform_(self.fc2.weight)
        nn.init.normal_(self.fc1.bias, std=1e-6)
        nn.init.normal_(self.fc2.bias, std=1e-6)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act_fn(x)
        x = self.layernorm(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x


def get_x1(x, y):
    """è®¡ç®— Frobenius èŒƒæ•°ï¼ˆæœ¬åœ°è®¡ç®—ç‰ˆæœ¬ï¼‰"""
    aij_matrix = torch.sub(y, x).half()
    x1 = torch.norm(aij_matrix, p='fro') 
    return x1


def get_x1_secure(party_features, other_party_features, weights, mpc_manager, batch_size=100):
    """
    ä½¿ç”¨åˆ†å— MPC è®¡ç®—èŒƒæ•°
    è®¡ç®— ||(A-B)W||_F
    
    Args:
        party_features: å½“å‰æ–¹çš„ç‰¹å¾
        other_party_features: æœªä½¿ç”¨ï¼ˆä¿æŒæ¥å£ä¸€è‡´ï¼‰
        weights: æƒé‡çŸ©é˜µ
        mpc_manager: MPC ç®¡ç†å™¨
        batch_size: æ‰¹å¤„ç†å¤§å°
    """
    party_id = mpc_manager.party_id
    
    # ä½¿ç”¨åˆ†å—è®¡ç®—
    if party_id == 0:
        # Party 0: æä¾› A å’Œ W
        result = mpc_manager.secure_matrix_multiply_batched(
            A=party_features,
            W=weights,
            batch_size=batch_size
        )
        x1 = torch.norm(result, p='fro')
        return x1
    else:
        # Party 1: æä¾› B
        result = mpc_manager.secure_matrix_multiply_batched(
            B=party_features,
            batch_size=batch_size
        )
        # KEY FIX: Party 1 returns a gradient-enabled zero to avoid blocking computation graph
        # Use part of result to maintain gradient connection
        dummy = torch.sum(result * 0.0)  # This maintains gradient graph but with value 0
        return dummy

    """
    ä½¿ç”¨åˆ†å— MPC è®¡ç®—èŒƒæ•°
    è®¡ç®— ||(A-B)W||_F
    
    Args:
        party_features: å½“å‰æ–¹çš„ç‰¹å¾
        other_party_features: æœªä½¿ç”¨ï¼ˆä¿æŒæ¥å£ä¸€è‡´ï¼‰
        weights: æƒé‡çŸ©é˜µ
        mpc_manager: MPC ç®¡ç†å™¨
        batch_size: æ‰¹å¤„ç†å¤§å°
    """
    party_id = mpc_manager.party_id
    
    # ğŸ”§ ä½¿ç”¨åˆ†å—è®¡ç®—
    if party_id == 0:
        # Party 0: æä¾› A å’Œ W
        result = mpc_manager.secure_matrix_multiply_batched(
            A=party_features,
            W=weights,
            batch_size=batch_size
        )
        x1 = torch.norm(result, p='fro')
        return x1
    else:
        # Party 1: æä¾› B
        result = mpc_manager.secure_matrix_multiply_batched(
            B=party_features,
            batch_size=batch_size
        )
        # Party 1 ä¸ä½¿ç”¨ç»“æœ
        return torch.tensor(0.0).requires_grad_(False)


class Tr(nn.Module):
    """åŸå§‹çš„ Tr ç±»ï¼ˆæœ¬åœ°è®¡ç®—ï¼‰"""
    def __init__(self, hid_dim, dropout):
        super(Tr, self).__init__()
        self.fc1 = Linear(hid_dim, hid_dim)

    def forward(self, x):
        z = x
        x = self.fc1(x)
        y = x
        x1 = get_x1(z, y)
        return x, x1


class SecureTr(nn.Module):
    """å¸¦ MPC çš„å®‰å…¨ Tr ç±»"""
    def __init__(self, hid_dim, dropout, party_id, use_mpc=True, batch_size=100):
        super(SecureTr, self).__init__()
        self.fc1 = Linear(hid_dim, hid_dim)
        self.party_id = party_id
        self.use_mpc = use_mpc
        self.batch_size = batch_size
        
        # åˆå§‹åŒ– MPC ç®¡ç†å™¨
        if use_mpc:
            self.mpc_manager = MPSPDZManager(party_id=party_id)
            print(f"[SecureTr] Party {party_id} initialized with MPC (batch_size={batch_size})")
        else:
            self.mpc_manager = None
            print(f"[SecureTr] Party {party_id} initialized WITHOUT MPC")
    
    def forward(self, x, compute_norm=True):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾
            compute_norm: æ˜¯å¦è®¡ç®—èŒƒæ•°ï¼ˆè®­ç»ƒæ—¶éœ€è¦ï¼‰
        
        Returns:
            x: å˜æ¢åçš„ç‰¹å¾
            x1: èŒƒæ•°å€¼ï¼ˆå¦‚æœ compute_norm=Trueï¼‰
        """
        z = x
        x = self.fc1(x)
        y = x
        
        if not compute_norm:
            return x, torch.tensor(0.0)
        
        if self.use_mpc and self.training:
            weights = self.fc1.weight.t()
            x1 = get_x1_secure(z, None, weights, self.mpc_manager, batch_size=self.batch_size)
            
            # KEY: get_x1_secure already returns appropriate values for both parties
            # Party 0 gets actual norm, Party 1 gets gradient-enabled zero
            return x, x1
        else:
            x1 = get_x1(z, y)
            return x, x1


class GMLP(nn.Module):
    """åŸå§‹çš„ GMLP ç±»"""
    def __init__(self, r, nfeat, nhid, nclass, dropout, drate):
        super(GMLP, self).__init__()
        self.nfeat = nfeat
        self.sum_l = sum(nfeat)
        self.nhid = nhid
        self.r = r
        
        self.tran = nn.ModuleList([
            Tr(self.nfeat[i], dropout).cuda() 
            for i in range(self.r)
        ])
        self.classifier = Mlp(self.sum_l, self.nhid, nclass, dropout)
        self.leaky_relu = nn.ModuleList([
            nn.LeakyReLU(drate) 
            for i in range(self.r)
        ])
    
    def forward(self, x):
        m = x
        x, _ = self.tran[0](m[0])
        x = self.leaky_relu[0](x)
        
        for i in range(1, self.r):
            y, _ = self.tran[i](m[i])
            x = torch.cat([x, self.leaky_relu[i](y)], dim=1)
        
        class_feature = self.classifier(x)
        class_logits = F.log_softmax(class_feature, dim=1)
        return class_logits


class SecureGMLP(nn.Module):
    """å¸¦ MPC çš„å®‰å…¨ GMLP ç±»"""
    def __init__(self, r, nfeat, nhid, nclass, dropout, drate, 
                 party_id, use_mpc=True, batch_size=100):
        super(SecureGMLP, self).__init__()
        self.nfeat = nfeat
        self.sum_l = sum(nfeat)
        self.nhid = nhid
        self.r = r
        self.party_id = party_id
        self.use_mpc = use_mpc
        self.batch_size = batch_size
        
        # ä½¿ç”¨ SecureTr æ›¿ä»£ Tr
        if use_mpc:
            self.tran = nn.ModuleList([
                SecureTr(self.nfeat[i], dropout, party_id, 
                        use_mpc=True, batch_size=batch_size).cuda() 
                for i in range(self.r)
            ])
        else:
            self.tran = nn.ModuleList([
                Tr(self.nfeat[i], dropout).cuda() 
                for i in range(self.r)
            ])
        
        self.classifier = Mlp(self.sum_l, self.nhid, nclass, dropout)
        self.leaky_relu = nn.ModuleList([
            nn.LeakyReLU(drate) 
            for i in range(self.r)
        ])
        
        print(f"[SecureGMLP] Initialized for Party {party_id}, MPC={use_mpc}, batch_size={batch_size}")
    
    def forward(self, x, compute_norm=True):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾åˆ—è¡¨
            compute_norm: æ˜¯å¦è®¡ç®—èŒƒæ•°
        
        Returns:
            class_logits: åˆ†ç±»è¾“å‡º
        """
        m = x
        x, _ = self.tran[0](m[0], compute_norm=compute_norm)
        x = self.leaky_relu[0](x)
        
        for i in range(1, self.r):
            y, _ = self.tran[i](m[i], compute_norm=compute_norm)
            x = torch.cat([x, self.leaky_relu[i](y)], dim=1)
        
        class_feature = self.classifier(x)
        class_logits = F.log_softmax(class_feature, dim=1)
        return class_logits